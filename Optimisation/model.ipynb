{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization\n",
    "\n",
    "***\n",
    "**Name**: Payoj Jain\n",
    "***\n",
    "\n",
    "## Goal\n",
    "\n",
    "The goal of this assignment is to use gradient based optimization algorithms to find the minum for the Rosenbrock function and to optmizie the 1 layer MLP network you built in **Assignment 4**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Optimizing the Rosenbrock function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More information can be found here https://en.wikipedia.org/wiki/Rosenbrock_function\n",
    "    \n",
    "**Note: For this assignment, we will choose a = 1 and b = 100**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed \n",
    "import random\n",
    "import unittest\n",
    "\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rosenbrock(a, b, x, y):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    a,b : parameters \n",
    "    x, y: inputs\n",
    "    \n",
    "    Outputs:\n",
    "    out: Rosenbrock function evaluated at x and y\n",
    "    \"\"\"\n",
    "    return (a-x)**2 + b*(y-x**2)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rosenbrock_grad(a, b, x, y):\n",
    "    \"\"\"\n",
    "    Calculate gradient of the rosenbrock function wrt x and y\n",
    "    \n",
    "    Inputs:\n",
    "    a, b: parameters\n",
    "    x, y: inputs\n",
    "    \n",
    "    Outputs:\n",
    "    grad_x, grad_y: Gradients wrt x and y\n",
    "    \"\"\"\n",
    "    grad_x = 2*(x - a) - 4*b*x*(y-x**2)\n",
    "    grad_y = 2*b*(y-x**2)\n",
    "    return grad_x, grad_y "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, you are given skeleton code for optimizing the rosenbrock function using various update rules. Following each function, there are a few function calls with specific hyperparameter choices. The outputs for these will be used to grade your work.\n",
    "\n",
    "Termination condition: \n",
    "\n",
    "1. Reached the n_epochs limit \n",
    "\n",
    "2. The change in value of the function at $x_t,y_t$ and $x_{t+1},y_{t+1}$ is <= tolerance \n",
    "\n",
    "All these functions share the same structure i.e apart from the update rule (and keeping track of past variables) very little changes across these functions.\n",
    "\n",
    "Note: since there is no randomness involved, we expect the outputs to closely match those of our implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rosenbrock_sgd(initial_x, initial_y, a, b, n_epochs, lr, tolerance):\n",
    "    \"\"\"\n",
    "    Use Vanilla SGD to optimize the Rosenbrock function\n",
    "    \n",
    "    Inputs:\n",
    "    initial_x, initial_y : Starting values\n",
    "    a, b : parameters\n",
    "    n_epochs : Maximum no of epochs \n",
    "    lr: Learning rate\n",
    "    tolerance: Tolerance for the error. Terminate if the function value does not change by atleast this much.\n",
    "    \n",
    "    Outputs:\n",
    "    final_x, final_y : Converged point after termination\n",
    "    stop_epoch: Epoch no at which we stop\n",
    "    \"\"\"\n",
    "    \n",
    "    final_x = initial_x\n",
    "    final_y = initial_y\n",
    "    change = function_value = rosenbrock(a,b,initial_x,initial_y)\n",
    "    stop_epoch = 0\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        if(change < tolerance):\n",
    "            print('SGD',final_x, final_y, stop_epoch)\n",
    "            return final_x,final_y,stop_epoch\n",
    "        \n",
    "        grad_x, grad_y = rosenbrock_grad(a,b,final_x,final_y)\n",
    "\n",
    "        final_x = final_x - lr*grad_x\n",
    "        final_y = final_y - lr*grad_y\n",
    "        \n",
    "        new_function_value = rosenbrock(a,b,final_x,final_y)\n",
    "        change = abs(new_function_value - function_value)\n",
    "        function_value = new_function_value\n",
    "        stop_epoch = epoch\n",
    "    return final_x,final_y,n_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rosenbrock_momentum(initial_x, initial_y, a, b, n_epochs, lr, mntm, nesterov, tolerance):\n",
    "    \"\"\"\n",
    "    Use momentum to optimize the Rosenbrock function\n",
    "    \n",
    "    Tip: While implementing nesterov update, you will need the gradient at the next step as well. \n",
    "        Instead, to simplify your implementation, you can use an alternative form of the nesterov \n",
    "        update which only uses the gradient at the current step.\n",
    "        Without nesterov, your update will be -> learning_rate*(gradient + momentum*grad_history)\n",
    "        With nesterov, you update will be -> lr*((1+mntm)*gradient + (mntm)^2 * grad_history)\n",
    "    \n",
    "    Inputs:\n",
    "    initial_x, initial_y : Starting values\n",
    "    a, b : parameters\n",
    "    n_epochs : Maximum no of epochs \n",
    "    lr: Learning rate\n",
    "    mntm: momentum factor\n",
    "    nesterov: True if nesterov update is to be used\n",
    "    tolerance: Tolerance for the error. Terminate if the error does not change by atleast this much.\n",
    "    \n",
    "    Outputs:\n",
    "    final_x, final_y : Converged point after termination\n",
    "    stop_epoch: Epoch no at which we stop\n",
    "    \"\"\"\n",
    "    \n",
    "    final_x = initial_x\n",
    "    final_y = initial_y\n",
    "    change = function_value = rosenbrock(a,b,initial_x,initial_y)\n",
    "    stop_epoch = 0\n",
    "    update_x, update_y = 0, 0\n",
    "\n",
    "    if nesterov:\n",
    "        gamma1 = 1 + mntm\n",
    "        gamma2 = mntm**2\n",
    "    else:\n",
    "        gamma1 = 1\n",
    "        gamma2 = mntm\n",
    "            \n",
    "    for epoch in range(n_epochs):\n",
    "        if(change < tolerance):\n",
    "            print('momentum','nesterov : ',nesterov,final_x, final_y, stop_epoch)\n",
    "            return final_x,final_y,stop_epoch\n",
    "        \n",
    "            \n",
    "        grad_x, grad_y = rosenbrock_grad(a,b,final_x,final_y)\n",
    "        \n",
    "        update_x = lr*(gamma1*grad_x + gamma2*update_x)\n",
    "        update_y = lr*(gamma1*grad_y + gamma2*update_y)\n",
    "                \n",
    "        final_x = final_x - update_x\n",
    "        final_y = final_y - update_y\n",
    "        \n",
    "        new_function_value = rosenbrock(a,b,final_x,final_y)\n",
    "        change = abs(new_function_value - function_value)\n",
    "        function_value = new_function_value\n",
    "        stop_epoch = epoch\n",
    "        \n",
    "    return final_x,final_y,n_epochs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rosenbrock_adagrad(initial_x, initial_y, a, b, n_epochs, lr, eps, tolerance):\n",
    "    \"\"\"\n",
    "    Use Adagrad to optimize the Rosenbrock function\n",
    "    \n",
    "    Inputs:\n",
    "    initial_x, initial_y : Starting values\n",
    "    a, b : parameters\n",
    "    n_epochs : Maximum no of epochs \n",
    "    lr: Learning rate\n",
    "    eps: The fudge factor (used in the denominator of the update to reduce numerical instability)\n",
    "    tolerance: Tolerance for the error. Terminate if the error does not change by atleast this much.\n",
    "    \n",
    "    Outputs:\n",
    "    final_x, final_y : Converged point after termination\n",
    "    stop_epoch: Epoch no at which we stop\n",
    "    \"\"\"\n",
    "    final_x = initial_x\n",
    "    final_y = initial_y\n",
    "    change = function_value = rosenbrock(a,b,initial_x,initial_y)\n",
    "    stop_epoch = 0\n",
    "    r_x, r_y = 0,0\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        if(change < tolerance):\n",
    "            print('adagrad',final_x, final_y, stop_epoch)\n",
    "            return final_x,final_y,stop_epoch\n",
    "        \n",
    "        grad_x, grad_y = rosenbrock_grad(a,b,final_x,final_y)\n",
    "        \n",
    "        r_x = r_x + grad_x*grad_x\n",
    "        r_y = r_y + grad_y*grad_y\n",
    "        \n",
    "        update_x = lr/((eps+r_x)**0.5)*grad_x\n",
    "        update_y = lr/((eps+r_y)**0.5)*grad_y\n",
    "        \n",
    "        final_x = final_x - update_x\n",
    "        final_y = final_y - update_y\n",
    "        \n",
    "        new_function_value = rosenbrock(a,b,final_x,final_y)\n",
    "        change = abs(new_function_value - function_value)\n",
    "        function_value = new_function_value\n",
    "        stop_epoch = epoch\n",
    "        \n",
    "    return final_x,final_y,n_epochs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rosenbrock_adadelta(initial_x, initial_y, a, b, n_epochs, rho, eps, tolerance):\n",
    "    \"\"\"\n",
    "    Use Adadelta to optimize the Rosenbrock function\n",
    "    \n",
    "    Inputs:\n",
    "    initial_x, initial_y : Starting values\n",
    "    a, b : parameters\n",
    "    n_epochs : Maximum no of epochs \n",
    "    rho: Averaging factor\n",
    "    eps: fudging factor (for numerical stability)\n",
    "    tolerance: Tolerance for the error. Terminate if the error does not change by atleast this much.\n",
    "    \n",
    "    Outputs:\n",
    "    final_x, final_y : Converged point after termination\n",
    "    stop_epoch: Epoch no at which we stop\n",
    "    \"\"\"\n",
    "    exponential_decay_update_x_avg = 0\n",
    "    exponential_decay_update_y_avg = 0\n",
    "    \n",
    "    exponential_decay_grad_x_avg = 0\n",
    "    exponential_decay_grad_y_avg = 0\n",
    "    \n",
    "    final_x = initial_x\n",
    "    final_y = initial_y\n",
    "    change = function_value = rosenbrock(a,b,initial_x,initial_y)\n",
    "    stop_epoch = 0\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        if(change < tolerance):\n",
    "            print('adadelta',final_x, final_y, stop_epoch)\n",
    "            return final_x,final_y,stop_epoch\n",
    "        \n",
    "        grad_x, grad_y = rosenbrock_grad(a,b,final_x,final_y)\n",
    "        \n",
    "        exponential_decay_grad_x_avg = rho*exponential_decay_grad_x_avg + (1-rho)*grad_x**2\n",
    "        exponential_decay_grad_y_avg = rho*exponential_decay_grad_y_avg + (1-rho)*grad_y**2\n",
    "        \n",
    "        rms_grad_x  = (exponential_decay_grad_x_avg + eps)**0.5\n",
    "        rms_grad_y  = (exponential_decay_grad_y_avg + eps)**0.5\n",
    "        \n",
    "        rms_update_x = (exponential_decay_update_x_avg + eps)**0.5\n",
    "        rms_update_y = (exponential_decay_update_y_avg + eps)**0.5\n",
    "        \n",
    "        update_x = (rms_update_x/rms_grad_x)*grad_x\n",
    "        update_y = (rms_update_y/rms_grad_y)*grad_y\n",
    "        \n",
    "        final_x = final_x - update_x\n",
    "        final_y = final_y - update_y\n",
    "        \n",
    "        exponential_decay_update_x_avg = rho*exponential_decay_update_x_avg + (1-rho)*update_x**2\n",
    "        exponential_decay_update_y_avg = rho*exponential_decay_update_y_avg + (1-rho)*update_y**2\n",
    "        \n",
    "        new_function_value = rosenbrock(a,b,final_x,final_y)\n",
    "        change = abs(new_function_value - function_value)\n",
    "        function_value = new_function_value\n",
    "        stop_epoch = epoch\n",
    "\n",
    "    return final_x,final_y,n_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rosenbrock_rmsprop(initial_x, initial_y, a, b, n_epochs, lr, rho, eps, tolerance):\n",
    "    \"\"\"\n",
    "    Use RMSprop to optimize the Rosenbrock function\n",
    "    \n",
    "    Inputs:\n",
    "    initial_x, initial_y : Starting values\n",
    "    a, b : parameters\n",
    "    n_epochs : Maximum no of epochs \n",
    "    lr:learning rate\n",
    "    rho: Averaging factor\n",
    "    eps: fudging factor (for numerical stability)\n",
    "    tolerance: Tolerance for the error. Terminate if the error does not change by atleast this much.\n",
    "    \n",
    "    Outputs:\n",
    "    final_x, final_y : Converged point after termination\n",
    "    stop_epoch: Epoch no at which we stop\n",
    "    \"\"\"\n",
    "    exponential_decay_grad_x_avg = 0\n",
    "    exponential_decay_grad_y_avg = 0\n",
    "    \n",
    "    final_x = initial_x\n",
    "    final_y = initial_y\n",
    "    change = function_value = rosenbrock(a,b,initial_x,initial_y)\n",
    "    stop_epoch = 0\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        if(change <= tolerance):\n",
    "            print('rmsprop',final_x, final_y, stop_epoch)\n",
    "            return final_x,final_y,stop_epoch\n",
    "        \n",
    "        grad_x, grad_y = rosenbrock_grad(a,b,final_x,final_y)\n",
    "        \n",
    "        exponential_decay_grad_x_avg = rho*exponential_decay_grad_x_avg + (1-rho)*grad_x**2\n",
    "        exponential_decay_grad_y_avg = rho*exponential_decay_grad_y_avg + (1-rho)*grad_y**2\n",
    "        \n",
    "        \n",
    "        update_x = (lr*grad_x/(eps + exponential_decay_grad_x_avg**0.5))\n",
    "        update_y = (lr*grad_y/(eps + exponential_decay_grad_y_avg**0.5))\n",
    "        \n",
    "        final_x = final_x - update_x\n",
    "        final_y = final_y - update_y\n",
    "        \n",
    "        new_function_value = rosenbrock(a,b,final_x,final_y)\n",
    "        change = abs(new_function_value - function_value)\n",
    "        function_value = new_function_value\n",
    "        stop_epoch = epoch\n",
    "\n",
    "    return final_x,final_y,n_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rosenbrock_adam(initial_x, initial_y, a, b, n_epochs, lr, beta1, beta2, eps, tolerance):\n",
    "    \"\"\"\n",
    "    Use Adam to optimize the Rosenbrock function\n",
    "    \n",
    "    Inputs:\n",
    "    initial_x, initial_y : Starting values\n",
    "    a, b : parameters\n",
    "    n_epochs : Maximum no of epochs \n",
    "    lr: learning rate\n",
    "    beta1, beta2: Averaging factors\n",
    "    eps: fudging factor (for numerical stability)\n",
    "    tolerance: Tolerance for the error. Terminate if the error does not change by atleast this much.\n",
    "    \n",
    "    Outputs:\n",
    "    final_x, final_y : Converged point after termination\n",
    "    stop_epoch: Epoch no at which we stop\n",
    "    \"\"\"\n",
    "    m_x, m_y = 0, 0\n",
    "    \n",
    "    v_x, v_y = 0, 0\n",
    "    \n",
    "    final_x = initial_x\n",
    "    final_y = initial_y\n",
    "    change = function_value = rosenbrock(a,b,initial_x,initial_y)\n",
    "    stop_epoch = 0\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        if(change < tolerance):\n",
    "            print('adam',final_x, final_y, stop_epoch)\n",
    "            return final_x,final_y,stop_epoch\n",
    "        \n",
    "        grad_x, grad_y = rosenbrock_grad(a,b,final_x,final_y)\n",
    "        \n",
    "        m_x = beta1*m_x + (1 - beta1)*grad_x\n",
    "        m_y = beta1*m_y + (1 - beta1)*grad_y\n",
    "        \n",
    "        v_x = beta2*v_x + (1 - beta2)*grad_x**2\n",
    "        v_y = beta2*v_y + (1 - beta2)*grad_y**2\n",
    "        \n",
    "        m_x_avg = m_x /(1 - beta1**(epoch+1))\n",
    "        m_y_avg = m_y /(1 - beta1**(epoch+1))\n",
    "        \n",
    "        v_x_avg = v_x /(1 - beta2**(epoch+1))\n",
    "        v_y_avg = v_y /(1 - beta2**(epoch+1))\n",
    "        \n",
    "        update_x = (lr/(v_x_avg**0.5+eps))*m_x_avg\n",
    "        update_y = (lr/(v_y_avg**0.5+eps))*m_y_avg\n",
    "        \n",
    "        final_x = final_x - update_x\n",
    "        final_y = final_y - update_y\n",
    "        \n",
    "        new_function_value = rosenbrock(a,b,final_x,final_y)\n",
    "        change = abs(new_function_value - function_value)\n",
    "        function_value = new_function_value\n",
    "        stop_epoch = epoch\n",
    "        \n",
    "    return final_x,final_y,n_epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unit Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "......"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adadelta 0.9320850940444184 0.8726938644568357 2341\n",
      "adagrad 0.9601029668996042 0.9217502990713288 8105\n",
      "adam 0.9882985356765367 0.9767314100355323 2191\n",
      "momentum nesterov :  True 0.975228639507472 0.9509702874332484 3253\n",
      "momentum nesterov :  False 0.9656473450926458 0.9323344634810155 5566\n",
      "rmsprop 0.9876101743117767 0.9768227299685097 3172\n",
      "SGD 0.965628504058544 0.932297997695398 5570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 6 tests in 0.095s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "class TestRosenBrock(unittest.TestCase):\n",
    "    def test_sgd(self):\n",
    "        final_x, final_y, stop_epoch = rosenbrock_sgd(0, 0, 1, 100, 1, 0.001, 1e-06)\n",
    "        self.assertAlmostEqual(final_x, 0.002, places = 4)\n",
    "        self.assertAlmostEqual(final_y, 0, places = 4)\n",
    "        self.assertAlmostEqual(stop_epoch, 1, places = 4)\n",
    "\n",
    "        final_x, final_y, stop_epoch = rosenbrock_sgd(0, 0, 1, 100, 5, 0.001, 1e-06)\n",
    "        self.assertAlmostEqual(final_x, 0.009959805751775453, places = 4)\n",
    "        self.assertAlmostEqual(final_y, 2.091e-05, places = 4)\n",
    "        self.assertAlmostEqual(stop_epoch, 5, places = 4)\n",
    "        \n",
    "        final_x, final_y, stop_epoch = rosenbrock_sgd(0, 0, 1, 100, 100000, 0.001, 1e-06)\n",
    "        self.assertAlmostEqual(final_x, 0.965628504058544, places = 4)\n",
    "        self.assertAlmostEqual(final_y, 0.932297997695398, places = 4)\n",
    "        self.assertAlmostEqual(stop_epoch,  5570, places = 4)\n",
    "    \n",
    "    def test_momentum(self):\n",
    "        final_x, final_y, stop_epoch = rosenbrock_momentum(0, 0, 1, 100, 1, 0.001, 0.95, True, 1e-06)\n",
    "        self.assertAlmostEqual(final_x, 0.0039, places = 4)\n",
    "        self.assertAlmostEqual(final_y, 0, places = 4)\n",
    "        self.assertAlmostEqual(stop_epoch, 1, places = 4)\n",
    "        \n",
    "        final_x, final_y, stop_epoch = rosenbrock_momentum(0, 0, 1, 100, 5, 0.001, 0.95, True, 1e-06)\n",
    "        self.assertAlmostEqual(final_x, 0.019358983472059735, places = 4)\n",
    "        self.assertAlmostEqual(final_y, 0.00013646707993741637, places = 4)\n",
    "        self.assertAlmostEqual(stop_epoch, 5, places = 4)\n",
    "        \n",
    "        final_x, final_y, stop_epoch = rosenbrock_momentum(0, 0, 1, 100, 100000, 0.001, 0.95, True, 1e-06)\n",
    "        self.assertAlmostEqual(final_x, 0.975228639507472, places = 4)\n",
    "        self.assertAlmostEqual(final_y, 0.9509702874332484, places = 4)\n",
    "        self.assertAlmostEqual(stop_epoch, 3253, places = 4)\n",
    "        \n",
    "        final_x, final_y, stop_epoch = rosenbrock_momentum(0, 0, 1, 100, 1, 0.001, 0.95, False, 1e-06)\n",
    "        self.assertAlmostEqual(final_x, 0.002, places = 4)\n",
    "        self.assertAlmostEqual(final_y, 0, places = 4)\n",
    "        self.assertAlmostEqual(stop_epoch, 1, places = 4)\n",
    "        \n",
    "        final_x, final_y, stop_epoch = rosenbrock_momentum(0, 0, 1, 100, 5, 0.001, 0.95, False, 1e-06)\n",
    "        self.assertAlmostEqual(final_x, 0.00996736498329375, places = 4)\n",
    "        self.assertAlmostEqual(final_y, 2.0949769804179422e-05, places = 4)\n",
    "        self.assertAlmostEqual(stop_epoch, 5, places = 4)\n",
    "        \n",
    "        final_x, final_y, stop_epoch = rosenbrock_momentum(0, 0, 1, 100, 100000, 0.001, 0.95, False, 1e-06)\n",
    "        self.assertAlmostEqual(final_x, 0.9656473450926458, places = 4)\n",
    "        self.assertAlmostEqual(final_y, 0.9323344634810155, places = 4)\n",
    "        self.assertAlmostEqual(stop_epoch, 5566, places = 4)\n",
    "\n",
    "    def test_adagrad(self):\n",
    "        final_x, final_y, stop_epoch = rosenbrock_adagrad(0, 0, 1, 100, 1, lr = 0.01, eps = 1e-04, tolerance = 1e-06)\n",
    "        self.assertAlmostEqual(final_x, 0.009999875002343702, places = 4)\n",
    "        self.assertAlmostEqual(final_y, 0, places = 4)\n",
    "        self.assertAlmostEqual(stop_epoch, 1, places = 4)\n",
    "\n",
    "        final_x, final_y, stop_epoch = rosenbrock_adagrad(0, 0, 1, 100, 5, lr = 0.01, eps = 1e-04, tolerance = 1e-06)\n",
    "        self.assertAlmostEqual(final_x, 0.0321538388386626, places = 4)\n",
    "        self.assertAlmostEqual(final_y, 0.0007748244054554124, places = 4)\n",
    "        self.assertAlmostEqual(stop_epoch, 5, places = 4)\n",
    "        \n",
    "        final_x, final_y, stop_epoch = rosenbrock_adagrad(0, 0, 1, 100, 100000, lr = 0.01, eps = 1e-04, tolerance = 1e-06)\n",
    "        self.assertAlmostEqual(final_x, 0.9601029668996042, places = 4)\n",
    "        self.assertAlmostEqual(final_y, 0.9217502990713288, places = 4)\n",
    "        self.assertAlmostEqual(stop_epoch, 8105, places = 4)\n",
    "        \n",
    "\n",
    "    def test_adadelta(self):\n",
    "        final_x, final_y, stop_epoch = rosenbrock_adadelta(0, 0, 1, 100, 1, rho = 0.95, eps = 1e-08, tolerance = 1e-06)\n",
    "        self.assertAlmostEqual(final_x, 0.0004472135843196183, places = 4)\n",
    "        self.assertAlmostEqual(final_y, 0, places = 4)\n",
    "        self.assertAlmostEqual(stop_epoch, 1, places = 4)\n",
    "        \n",
    "        final_x, final_y, stop_epoch = rosenbrock_adadelta(0, 0, 1, 100, 5, rho = 0.95, eps = 1e-08, tolerance = 1e-06)\n",
    "        self.assertAlmostEqual(final_x, 0.0022774214255509655, places = 4)\n",
    "        self.assertAlmostEqual(final_y, -0.00020407559768761523, places = 4)\n",
    "        self.assertAlmostEqual(stop_epoch, 5, places = 4)\n",
    "        \n",
    "        final_x, final_y, stop_epoch = rosenbrock_adadelta(0, 0, 1, 100, 10000, rho = 0.95, eps = 1e-08, tolerance = 1e-06)\n",
    "        self.assertAlmostEqual(final_x, 0.9320850940444182, places = 4)\n",
    "        self.assertAlmostEqual(final_y, 0.8726938644568353, places = 4)\n",
    "        self.assertAlmostEqual(stop_epoch, 2341, places = 4)\n",
    "        \n",
    "    def test_rmsprop(self):\n",
    "        final_x, final_y, stop_epoch = rosenbrock_rmsprop(0, 0, 1, 100, 1, lr = 0.001, rho = 0.95, eps = 1e-08, tolerance = 1e-06)\n",
    "        self.assertAlmostEqual(final_x, 0.004472135843196183, places = 4)\n",
    "        self.assertAlmostEqual(final_y, 0, places = 4)\n",
    "        self.assertAlmostEqual(stop_epoch, 1, places = 4)\n",
    "        \n",
    "        final_x, final_y, stop_epoch = rosenbrock_rmsprop(0, 0, 1, 100, 5, lr = 0.001, rho = 0.95, eps = 1e-08, tolerance = 1e-06)\n",
    "        self.assertAlmostEqual(final_x, 0.01471318674752176, places = 4)\n",
    "        self.assertAlmostEqual(final_y, 0.00016279213371358814, places = 4)\n",
    "        self.assertAlmostEqual(stop_epoch, 5, places = 4)\n",
    "        \n",
    "        final_x, final_y, stop_epoch = rosenbrock_rmsprop(0, 0, 1, 100, 100000, lr = 0.001, rho = 0.95, eps = 1e-08, tolerance = 1e-08)\n",
    "        self.assertAlmostEqual(final_x, 0.9876128078500009, places = 4)\n",
    "        self.assertAlmostEqual(final_y,  0.9768279433532969, places = 4)\n",
    "        self.assertAlmostEqual(stop_epoch, 3172, places = 4)\n",
    "\n",
    "    def test_adam(self):\n",
    "        final_x, final_y, stop_epoch = rosenbrock_adam(0, 0, 1, 100, 1, lr = 0.001, beta1 = 0.9, beta2 = 0.999, eps = 1e-08, tolerance = 1e-06)\n",
    "        self.assertAlmostEqual(final_x, 0.000999999995, places = 4)\n",
    "        self.assertAlmostEqual(final_y, 0, places = 4)\n",
    "        self.assertAlmostEqual(stop_epoch, 1, places = 4)\n",
    "        \n",
    "        final_x, final_y, stop_epoch = rosenbrock_adam(0, 0, 1, 100, 5, lr = 0.001, beta1 = 0.9, beta2 = 0.999, eps = 1e-08, tolerance = 1e-06)\n",
    "        self.assertAlmostEqual(final_x, 0.004999562994105255, places = 4)\n",
    "        self.assertAlmostEqual(final_y, -0.0005982751561081573, places = 4)\n",
    "        self.assertAlmostEqual(stop_epoch, 5, places = 4)\n",
    "        \n",
    "        final_x, final_y, stop_epoch = rosenbrock_adam(0, 0, 1, 100, 10000, lr = 0.001, beta1 = 0.9, beta2 = 0.999, eps = 1e-08, tolerance = 1e-06)\n",
    "        self.assertAlmostEqual(final_x, 0.9882985356765371, places = 4)\n",
    "        self.assertAlmostEqual(final_y, 0.9767314100355338, places = 4)\n",
    "        self.assertAlmostEqual(stop_epoch, 2191, places = 4)\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=['first-arg-is-ignored'], exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensor import Tensor\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Implementing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "class MLP(object):\n",
    "    def __init__(self, n_in, n_hid, n_out):\n",
    "\n",
    "        self.l1 = Tensor(np.random.randn(n_in, n_hid))\n",
    "        self.l2 = Tensor(np.random.randn(n_hid, n_out))\n",
    "        \n",
    "        self.state = defaultdict(dict)\n",
    "        \n",
    "        # Initialize an empty dictionary (per pair of weights and biases) for future use\n",
    "        # This dictionary will store various histories needed int he algorithms to be implemented\n",
    "        for i in [1, 2]:\n",
    "            self.state[str(i)] = {}\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "            Implement the forward pass.\n",
    "\n",
    "            params:\n",
    "            X (NxM Tensor):\n",
    "                M dimensional input to be feed forward to the network.\n",
    "\n",
    "            returns:\n",
    "\n",
    "            y_hat (1xN Tensor): activations of the output layer.\n",
    "        \"\"\"\n",
    "\n",
    "        self.act_l1 = X.dot(self.l1).relu()\n",
    "        self.act_l2 = self.act_l1.dot(self.l2)\n",
    "\n",
    "        return self.act_l2\n",
    "\n",
    "    def update(self, opt):\n",
    "        \"\"\"\n",
    "            Implement the update rule for network weights and biases.\n",
    "\n",
    "            return:\n",
    "\n",
    "            none.\n",
    "        \"\"\"\n",
    "        \n",
    "        grad_1, grad_2 = opt.update_grad(self)\n",
    "        \n",
    "        self.l1.value += grad_1.value\n",
    "        self.l2.value += grad_2.value\n",
    "        \n",
    "    def get_state_accum(self, param, state_dict, name):\n",
    "        \"\"\"\n",
    "        Return a particular variable associated with a layer. If it has not been initialized \n",
    "        (i.e. on the first iteration), then create that variable with the same dimention as param\n",
    "\n",
    "        Example usage: get_state_accum(W, nn.state[str(layer)], 'W_grad_history') would return the \n",
    "                       gradient in the past iteration (assuming you update W_grad_history in the dictionary every\n",
    "                       epoch)\n",
    "\n",
    "        Inputs:\n",
    "        state_dict: dictionary to be queried \n",
    "        name: property of interest\n",
    "        param: If state_dict[name] is being accessed the first time, we initialize it to be a \n",
    "               vector with the same dimesions as param\n",
    "        \"\"\"\n",
    "        if name not in state_dict:\n",
    "            state_dict[name] = np.zeros_like(param)\n",
    "        return state_dict[name]\n",
    "    \n",
    "    def get_layer_params(self, layer):\n",
    "        \"\"\"\n",
    "        Return aparameters of a particular layer\n",
    "        \"\"\"\n",
    "        if layer == 1:\n",
    "            return self.l1\n",
    "        else:\n",
    "            return self.l2\n",
    "        \n",
    "    def get_layer_grad(self, layer):\n",
    "        \"\"\" Return gradient of a particular layer \"\"\"\n",
    "        if layer == 1:\n",
    "            return self.l1.grad\n",
    "        else:\n",
    "            return self.l2.grad\n",
    "\n",
    "    def zero_grad(self):\n",
    "        \"\"\"\n",
    "            Resets gradients for each layer defined in\n",
    "            the constructor.\n",
    "        \"\"\"\n",
    "        self.l1.zero_grad()\n",
    "        self.l2.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class optimizer():\n",
    "    def __init__(self, update_rule, **kwargs):\n",
    "        \"\"\"\n",
    "        Set self.update \n",
    "        Set parameters from kwargs(variable keyword parameters) according to update type\n",
    "        \"\"\"\n",
    "        self.update = update_rule\n",
    "        \n",
    "        if self.update == \"sgd\":\n",
    "            self.lr = kwargs['lr']\n",
    "        elif self.update == \"momentum\":\n",
    "            self.lr = kwargs['lr']\n",
    "            self.mntm = kwargs['mntm']\n",
    "            self.nesterov = kwargs['nesterov']\n",
    "        elif self.update == \"adagrad\":\n",
    "            self.lr = kwargs['lr']\n",
    "            self.eps = kwargs['eps']\n",
    "        elif self.update == \"adadelta\":\n",
    "            self.rho = kwargs['rho']\n",
    "            self.eps = kwargs['eps']\n",
    "        elif self.update == \"rmsprop\":\n",
    "            self.lr = kwargs['lr']\n",
    "            self.rho = kwargs['rho']\n",
    "            self.eps = kwargs['eps']\n",
    "        elif self.update == \"adam\":\n",
    "            self.lr = kwargs['lr']\n",
    "            self.beta1 = kwargs['beta1']\n",
    "            self.beta2 = kwargs['beta2']\n",
    "            self.eps = kwargs['eps']\n",
    "    \n",
    "    def update_grad(self, model):\n",
    "        \"\"\"\n",
    "        Returns proper update according to the string in self.update\n",
    "        \n",
    "        Input:\n",
    "        model: NN model\n",
    "        \n",
    "        Output:\n",
    "        l1: Update for layer 1\n",
    "        l2: update for layer 2\n",
    "        \"\"\"\n",
    "        if self.update == 'sgd':\n",
    "            l1 = self.update_sgd(1, model, self.lr)\n",
    "            l2 = self.update_sgd(2, model, self.lr)\n",
    "        elif self.update == 'momentum':\n",
    "            l1 = self.update_momentum(1, model, self.lr, self.mntm, self.nesterov)\n",
    "            l2 = self.update_momentum(2, model, self.lr, self.mntm, self.nesterov)\n",
    "        elif self.update == 'adagrad':\n",
    "            l1 = self.update_adagrad(1, model, self.lr, self.eps)\n",
    "            l2 = self.update_adagrad(2, model, self.lr, self.eps)\n",
    "        elif self.update == 'adadelta':\n",
    "            l1 = self.update_adadelta(1, model, self.rho, self.eps)\n",
    "            l2 = self.update_adadelta(2, model, self.rho, self.eps)\n",
    "        elif self.update == 'rmsprop':\n",
    "            l1 = self.update_rmsprop(1, model, self.lr, self.rho, self.eps)\n",
    "            l2 = self.update_rmsprop(2, model, self.lr, self.rho, self.eps)\n",
    "        elif self.update == 'adam':\n",
    "            l1 = self.update_adam(1, model, self.lr, self.beta1, self.beta2, self.eps)\n",
    "            l2 = self.update_adam(2, model, self.lr, self.beta1, self.beta2, self.eps)\n",
    "            \n",
    "        return l1, l2\n",
    "        \n",
    "    def update_sgd(self, layer, model, lr):\n",
    "        \"\"\"\n",
    "        Update function for sgd.\n",
    "        Make sure you update the moving averages for 'layer' inside this function. \n",
    "    \n",
    "        Inputs:\n",
    "        layer: layer for which the update has to be returned\n",
    "        model: NN model to be updated (you need this to access 'state' and update histories)\n",
    "        lr: learning rate\n",
    "\n",
    "        Outputs:\n",
    "        update (Tensor)\n",
    "        \"\"\"\n",
    "        name = 'sgd'\n",
    "        grad = model.get_layer_grad(layer)\n",
    "        update = -lr*grad\n",
    "        model.state[str(layer)][name] = update\n",
    "        return Tensor(update)\n",
    "    \n",
    "    def update_momentum(self, layer, model, lr, mntm, nesterov):\n",
    "        \"\"\"\n",
    "        Update function for momentum.\n",
    "        Make sure you update the past gradients for 'layer' inside this function.\n",
    "\n",
    "        Tip: While implementing nesterov update, you will need the gradient at the next step as well. \n",
    "            Instead, to simplify your implementation, you can use an alternative form of the nesterov \n",
    "            update which only uses the gradient at the current step.\n",
    "            Without nesterov, your update will be -> learning_rate*(gradient + momentum*grad_history)\n",
    "            With nesterov, you update will be -> lr*((1+mntm)*gradient + (mntm)^2 * grad_history)\n",
    "            \n",
    "        Inputs:\n",
    "        layer: layer for which the update has to be returned\n",
    "        model: NN model to be updated (you need this to access 'state' and update histories)\n",
    "        lr: learning rate\n",
    "        mntm: momentum factor\n",
    "        nesterov: if True, function returns nesterov accelerated update\n",
    "\n",
    "        Outputs:\n",
    "        update (Tensor)\n",
    "        \"\"\"\n",
    "        if nesterov:\n",
    "            name = 'momentum nesterov'\n",
    "            gamma1 = 1+mntm\n",
    "            gamma2 = mntm**2\n",
    "        else:\n",
    "            name = 'momentum without nesterov'\n",
    "            gamma1 = 1\n",
    "            gamma2 = mntm\n",
    "            \n",
    "        grad = model.get_layer_grad(layer)\n",
    "        \n",
    "        update_history = model.get_state_accum(grad, model.state[str(layer)], name)\n",
    "        update = -lr*(gamma1*grad + gamma2*update_history)\n",
    "        model.state[str(layer)][name] = update\n",
    "        \n",
    "        return Tensor(update)\n",
    "    \n",
    "    def update_adagrad(self, layer, model, lr, eps):\n",
    "        \"\"\"\n",
    "        Update function for adadelta.\n",
    "        Make sure you update the moving averages for 'layer' inside this function. \n",
    "    \n",
    "        Inputs:\n",
    "        layer: layer for which the update has to be returned\n",
    "        model: NN model to be updated (you need this to access 'state' and update histories)\n",
    "        lr: learning rate\n",
    "        eps : fudge factor (for numerical stability)\n",
    "\n",
    "        Outputs:\n",
    "        update (Tensor)\n",
    "        \"\"\"\n",
    "        '''\n",
    "        r_x = r_x + grad_x*grad_x\n",
    "        \n",
    "        update_x = lr/((eps+r_x)**0.5)*grad_x\n",
    "        \n",
    "        '''\n",
    "        name = 'adagrad'\n",
    "        grad = model.get_layer_grad(layer)\n",
    "        \n",
    "        r_history = model.get_state_accum(grad, model.state[str(layer)], name)\n",
    "        r = r_history + grad**2\n",
    "        \n",
    "        model.state[str(layer)][name] = r\n",
    "        \n",
    "        update = -lr/((eps+r)**0.5)*grad\n",
    "\n",
    "        return Tensor(update)\n",
    "        \n",
    "    def update_adadelta(self, layer, model, rho, eps):\n",
    "        \"\"\"\n",
    "        Update function for adadelta.\n",
    "        Make sure you update the moving averages for 'layer' inside this function. \n",
    "    \n",
    "        Inputs:\n",
    "        layer: layer for which the update has to be returned\n",
    "        model: NN model to be updated (you need this to access 'state' and update histories)\n",
    "        rho: averaging factor\n",
    "        eps : fudge factor (for numerical stability)\n",
    "\n",
    "        Outputs:\n",
    "        update (Tensor)\n",
    "        \"\"\"\n",
    "        '''\n",
    "        exponential_decay_grad_x_avg = rho*exponential_decay_grad_x_avg + (1-rho)*grad_x**2\n",
    "        \n",
    "        rms_grad_x  = (exponential_decay_grad_x_avg + eps)**0.5\n",
    "\n",
    "        \n",
    "        rms_update_x = (exponential_decay_update_x_avg + eps)**0.5\n",
    "        \n",
    "        update_x = (rms_update_x/rms_grad_x)*grad_x\n",
    "        \n",
    "        exponential_decay_update_x_avg = rho*exponential_decay_update_x_avg + (1-rho)*update_x**2\n",
    "\n",
    "        \n",
    "        '''\n",
    "        update_name = 'adadelta_update'\n",
    "        grad_name = 'adadelta_grad'\n",
    "        \n",
    "        grad = model.get_layer_grad(layer)\n",
    "        \n",
    "        update_history = model.get_state_accum(grad, model.state[str(layer)], update_name)\n",
    "        grad_history = model.get_state_accum(grad,model.state[str(layer)],grad_name)\n",
    "        \n",
    "        grad_history = rho*grad_history + (1-rho)*grad**2\n",
    "        \n",
    "        rms_grad = (grad_history + eps)**0.5\n",
    "        rms_update = (update_history + eps)**0.5\n",
    "        \n",
    "        update = -(rms_update/rms_grad)*grad\n",
    "        \n",
    "        update_history = rho*update_history + (1-rho)*update**2\n",
    "        \n",
    "        model.state[str(layer)][update_name] = update_history\n",
    "        model.state[str(layer)][grad_name] = grad_history\n",
    "        \n",
    "        return Tensor(update)\n",
    "    \n",
    "    def update_rmsprop(self, layer, model, lr, rho, eps):\n",
    "        \"\"\"\n",
    "        Update function for rmsprop.\n",
    "        Make sure you update the moving averages for 'layer' inside this function. \n",
    "    \n",
    "        Inputs:\n",
    "        layer: layer for which the update has to be returned\n",
    "        model: NN model to be updated (you need this to access 'state' and update histories)\n",
    "        lr: learning rate\n",
    "        rho : averaging factor\n",
    "        eps : fudge factor (for numerical stability)\n",
    "\n",
    "        Outputs:\n",
    "        update (Tensor)\n",
    "        \"\"\"\n",
    "        '''\n",
    "        grad_x, grad_y = rosenbrock_grad(a,b,final_x,final_y)\n",
    "        \n",
    "        exponential_decay_grad_x_avg = rho*exponential_decay_grad_x_avg + (1-rho)*grad_x**2\n",
    "        \n",
    "        update_x = (lr*grad_x/(eps + exponential_decay_grad_x_avg**0.5))\n",
    "\n",
    "        '''\n",
    "        \n",
    "        update_name = 'rmsprop'\n",
    "        \n",
    "        grad = model.get_layer_grad(layer)\n",
    "        \n",
    "        update_history = model.get_state_accum(grad, model.state[str(layer)], update_name)\n",
    "        \n",
    "        update_history = rho*update_history + (1-rho)*grad**2\n",
    "        \n",
    "        model.state[str(layer)][update_name] = update_history\n",
    "        \n",
    "        update = -(lr*grad)/(eps+update_history**0.5)\n",
    "        \n",
    "        return Tensor(update)\n",
    "    \n",
    "    def update_adam(self, layer, model, lr, beta1, beta2, eps):\n",
    "        \"\"\"\n",
    "        Update function for adam.\n",
    "        Make sure you update the moving averages for 'layer' inside this function. \n",
    "    \n",
    "        Inputs:\n",
    "        layer: layer for which the update has to be returned\n",
    "        model: NN model to be updated (you need this to access 'state' and update histories)\n",
    "        lr: learning rate\n",
    "        beta1, beta2 : averaging factors\n",
    "        eps : fudge factor (for numerical stability)\n",
    "\n",
    "        Outputs:\n",
    "        update (Tensor)\n",
    "        \"\"\"\n",
    "        '''\n",
    "        grad_x, grad_y = rosenbrock_grad(a,b,final_x,final_y)\n",
    "        \n",
    "        m_x = beta1*m_x + (1 - beta1)*grad_x\n",
    "        \n",
    "        v_x = beta2*v_x + (1 - beta2)*grad_x**2\n",
    "        \n",
    "        m_x_avg = m_x /(1 - beta1**(epoch+1))\n",
    "        \n",
    "        v_x_avg = v_x /(1 - beta2**(epoch+1))\n",
    "        \n",
    "        update_x = (lr/(v_x_avg**0.5+eps))*m_x_avg\n",
    "        '''\n",
    "        \n",
    "        m_name = 'adam_m'\n",
    "        v_name = 'adam_v'\n",
    "        exp_name = 'epoch_num'\n",
    "        grad = model.get_layer_grad(layer)\n",
    "        \n",
    "        m_history = model.get_state_accum(grad, model.state[str(layer)], m_name)\n",
    "        v_history = model.get_state_accum(grad, model.state[str(layer)], v_name)\n",
    "        \n",
    "        m_history = beta1*m_history + (1 - beta1)*grad\n",
    "        v_history = beta2*v_history + (1 - beta2)*grad**2\n",
    "        \n",
    "        exp = model.get_state_accum(1, model.state[str(layer)], exp_name)\n",
    "        exp += 1\n",
    "        m_avg = m_history/(1 - beta1**exp)\n",
    "        v_avg = v_history/(1 - beta2**exp)\n",
    "        \n",
    "        model.state[str(layer)][m_name] = m_history\n",
    "        model.state[str(layer)][v_name] = v_history\n",
    "        model.state[str(layer)][exp_name] = exp\n",
    "        \n",
    "        update = (-lr/(v_avg**0.5 + eps))*m_avg\n",
    "        \n",
    "        return Tensor(update)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Train the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, X, y, epochs, batch_size, opt):\n",
    "    \"\"\"\n",
    "    Implement the train loop.\n",
    "    \n",
    "    params:\n",
    "    model (MLP): 1-hidden-layer MLP model to be trained.\n",
    "    X (Nx5 ndarray): Training inputs.\n",
    "    y (Nx1 ndarray): Groundtruth labels.\n",
    "    epochs (int): number of epochs for training\n",
    "    batch_size (int)\n",
    "    opt: optimizer object\n",
    "    \n",
    "    returns:\n",
    "    list of errors (one for each epoch)\n",
    "    \"\"\"\n",
    "    # For each minibatch in each epoch, \n",
    "    # 1. do a forward pass on the minibatch\n",
    "    # 2. Compute Loss on minibatch\n",
    "    # 3. Do a backward pass to get gradients\n",
    "    # 4. Update parameters\n",
    "    # 5. Compute error over epoch\n",
    "    \n",
    "\n",
    "    from sklearn.utils import shuffle\n",
    "    mean_squared_error = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        X, y = shuffle(X, y)        \n",
    "        epoch_error = 0\n",
    "\n",
    "        for batch in range(0,X.shape[0],batch_size):\n",
    "            \n",
    "            X_train = Tensor(X[batch:batch+batch_size])\n",
    "            y_train = Tensor(y[batch:batch+batch_size])\n",
    "            \n",
    "            y_hat = model.forward(X_train)\n",
    "            \n",
    "            model.zero_grad()\n",
    "            \n",
    "            mse = (y_train - y_hat).pow(2).mean()\n",
    "            \n",
    "            mse.backward()\n",
    "            \n",
    "            model.update(opt)\n",
    "            \n",
    "            epoch_error += mse.value\n",
    "            \n",
    "        mean_error = epoch_error/(X.shape[0]/batch_size)\n",
    "        mean_squared_error.append(mean_error)\n",
    "    return mean_squared_error\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Usage and Tests\n",
    "***\n",
    "\n",
    "**Part 3.A**  \n",
    "\n",
    "Create a model with $I = 5$ inputs, $H = 100$ and $O = 1$ output neuron. Train the model for 100 epochs using the dataset below. Plot the MSE loss for each epoch.\n",
    "\n",
    "Once you have each optimizer working, which performs best given their initial hyperparameters? Does changing one or more hyperpameters yield better performance for a particular optimizer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __ADAGRAD__ seems to perform the best in the given initial hyperparameters. It converges fast and also outputs the minimum loss among other optimisers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = np.load(\"X.npy\"), np.load(\"y.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.26101524]\n"
     ]
    }
   ],
   "source": [
    "model = MLP(X.shape[1], 100, 1)\n",
    "opt = optimizer(\"sgd\", lr = 0.001)\n",
    "mse_sgd = train(model, X, y, 100, 100, opt)\n",
    "print(mse_sgd[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.36574325]\n"
     ]
    }
   ],
   "source": [
    "model = MLP(X.shape[1], 100, 1)\n",
    "opt = optimizer(\"momentum\", lr = 0.001, mntm = 0.95, nesterov = False)\n",
    "mse_mntm = train(model, X, y, 100, 100, opt)\n",
    "print(mse_mntm[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.22690645]\n"
     ]
    }
   ],
   "source": [
    "model = MLP(X.shape[1], 100, 1)\n",
    "opt = optimizer(\"momentum\", lr = 0.001, mntm = 0.95, nesterov = True)\n",
    "mse_nesterov = train(model, X, y, 100, 100, opt)\n",
    "print(mse_nesterov[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.17254225]\n"
     ]
    }
   ],
   "source": [
    "model = MLP(X.shape[1], 100, 1)\n",
    "opt = optimizer(\"adagrad\", lr = 0.1, eps = 1e-06)\n",
    "mse_adagrad = train(model, X, y, 100, 100, opt)\n",
    "print(mse_adagrad[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.25607634]\n"
     ]
    }
   ],
   "source": [
    "model = MLP(X.shape[1], 100, 1)\n",
    "opt = optimizer(\"adadelta\", rho = 0.9, eps = 1e-06)\n",
    "mse_adadelta = train(model, X, y, 100, 100, opt)\n",
    "print(mse_adadelta[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.23562662]\n"
     ]
    }
   ],
   "source": [
    "model = MLP(X.shape[1], 100, 1)\n",
    "opt = optimizer(\"rmsprop\", lr = 0.001, rho = 0.9, eps = 1e-06)\n",
    "mse_rmsprop = train(model, X, y, 100, 100, opt)\n",
    "print(mse_rmsprop[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.29285499]\n"
     ]
    }
   ],
   "source": [
    "model = MLP(X.shape[1], 100, 1)\n",
    "opt = optimizer(\"adam\", lr = 0.001, beta1 = 0.9, beta2 = 0.999, eps = 1e-04)\n",
    "mse_adam = train(model, X, y, 100, 100, opt)\n",
    "print(mse_adam[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x115480208>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1151673c8>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12,5))\n",
    "plt.plot(range(100), mse_sgd, label = 'SGD')\n",
    "plt.plot(range(100), mse_mntm, label = 'Momentum')\n",
    "plt.plot(range(100), mse_nesterov, label = 'Nesterov')\n",
    "plt.plot(range(100), mse_adagrad, label = 'ADAGRAD')\n",
    "plt.plot(range(100), mse_adadelta, label = 'ADADELTA')\n",
    "plt.plot(range(100), mse_rmsprop, label = 'RMSPROP')\n",
    "plt.plot(range(100), mse_adam, label = 'ADAM')\n",
    "plt.xlim(0,25)\n",
    "# plt.ylim(0,300)\n",
    "# plt.yscale('log')\n",
    "plt.title('MSE loss for each epoch. batch size = 100')\n",
    "plt.xlabel('Number of epochs')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrs = [0.0001, 0.001, 0.005]\n",
    "min_sgd_mse = 10\n",
    "for lr in lrs:\n",
    "    model = MLP(X.shape[1], 100, 1)\n",
    "    opt = optimizer(\"sgd\", lr = lr)\n",
    "    mse_sgd = train(model, X, y, 100, 100, opt)\n",
    "    if(mse_sgd[-1] < min_sgd_mse):\n",
    "        lr_sgd = lr\n",
    "        min_sgd_mse = mse_sgd[-1]\n",
    "        sgd_mse = mse_sgd\n",
    "\n",
    "min_mntm_mse = 10\n",
    "for lr in lrs:\n",
    "    mntms = [0.9, 0.95]\n",
    "    for mntm in mntms:\n",
    "        model = MLP(X.shape[1], 100, 1)\n",
    "        opt = optimizer(\"momentum\", lr = lr, mntm = mntm, nesterov = False)\n",
    "        mse_mntm = train(model, X, y, 100, 100, opt)\n",
    "        if(mse_mntm[-1] < min_mntm_mse):\n",
    "            lr_mntm = lr\n",
    "            mntm_mntm = mntm\n",
    "            min_mntm_mse = mse_mntm[-1]\n",
    "            mntm_mse = mse_mntm\n",
    "\n",
    "min_nesterov_mse = 10\n",
    "for lr in lrs:\n",
    "    mntms = [0.9, 0.95]\n",
    "    for mntm in mntms:\n",
    "        model = MLP(X.shape[1], 100, 1)\n",
    "        opt = optimizer(\"momentum\", lr = lr, mntm = mntm, nesterov = True)\n",
    "        mse_nesterov = train(model, X, y, 100, 100, opt)\n",
    "        if(mse_nesterov[-1] < min_nesterov_mse):\n",
    "            lr_nesterov = lr\n",
    "            nesterov_mntm = mntm\n",
    "            min_nesterov_mse = mse_nesterov[-1]\n",
    "            nesterov_mse = mse_nesterov\n",
    "\n",
    "min_adagrad_mse = 10\n",
    "for lr in lrs:  \n",
    "    model = MLP(X.shape[1], 100, 1)\n",
    "    opt = optimizer(\"adagrad\", lr = lr, eps = 1e-06)\n",
    "    mse_adagrad = train(model, X, y, 100, 100, opt)\n",
    "    if(mse_adagrad[-1] < min_adagrad_mse):\n",
    "        lr_adagrad = lr\n",
    "        min_adagrad_mse = mse_adagrad[-1]\n",
    "        adagrad_mse = mse_adagrad\n",
    "\n",
    "\n",
    "min_adadelta_mse = 10\n",
    "rhos = [0.85, 0.9, 0.95]\n",
    "for rho in rhos:\n",
    "    model = MLP(X.shape[1], 100, 1)\n",
    "    opt = optimizer(\"adadelta\", rho = rho, eps = 1e-06)\n",
    "    mse_adadelta = train(model, X, y, 100, 100, opt)\n",
    "    if(mse_adadelta[-1] < min_adadelta_mse):\n",
    "        rho_adadelta = rho\n",
    "        min_adadelta_mse = mse_adadelta[-1]\n",
    "        adadelta_mse = mse_adadelta\n",
    "\n",
    "min_rmsprop_mse = 10\n",
    "for lr in lrs:\n",
    "    rhos = [0.85, 0.9, 0.95]\n",
    "    for rho in rhos:\n",
    "        model = MLP(X.shape[1], 100, 1)\n",
    "        opt = optimizer(\"rmsprop\", lr = lr, rho = rho, eps = 1e-06)\n",
    "        mse_rmsprop = train(model, X, y, 100, 100, opt)\n",
    "        if(mse_rmsprop[-1] < min_rmsprop_mse):\n",
    "            rho_rmsprop = rho\n",
    "            lr_rmsprop = lr\n",
    "            min_rmsprop_mse = mse_rmsprop[-1]\n",
    "            rmsprop_mse = mse_rmsprop\n",
    "\n",
    "min_adam_mse = 10\n",
    "for lr in lrs:\n",
    "    rhos = [0.85, 0.9, 0.95]\n",
    "    beta1s = [0.85, 0.9, 0.95]\n",
    "    beta2s = [0.95, 0.98, 0.999]\n",
    "    for beta1 in beta1s:\n",
    "        for beta2 in beta2s:\n",
    "            model = MLP(X.shape[1], 100, 1)\n",
    "            opt = optimizer(\"adam\", lr = lr, beta1 = beta1, beta2 = beta2, eps = 1e-04)\n",
    "            mse_adam = train(model, X, y, 100, 100, opt)\n",
    "            if(mse_adam[-1] < min_adam_mse):\n",
    "                beta1_adam = beta1\n",
    "                beta2_adam = beta2\n",
    "                lr_adam = lr\n",
    "                min_adam_mse = mse_adam[-1]\n",
    "                adam_mse = mse_adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimal Hyperparameters for each optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD loss after 100 epochs:      0.177225235148 for lr:  0.005\n",
      "Momentum loss after 100 epochs: 0.191406097321 for lr:  0.005 , mntm:  0.95\n",
      "Nesterov loss after 100 epochs: 0.1475822108 for lr:  0.005 , mntm:  0.95\n",
      "ADAGRAD loss after 100 epochs:  0.572693580944  for lr:  0.005\n",
      "ADADELTA loss after 100 epochs: 0.223346379035 for rho:  0.85\n",
      "RMSPROP loss after 100 epochs:  0.178928022109 for lr:  0.001 , rho:  0.95\n",
      "ADAM loss after 100 epochs:     0.137500867895 for lr:  0.005 , beta1:  0.95 , beta2:  0.95\n"
     ]
    }
   ],
   "source": [
    "print('SGD loss after 100 epochs:     ', min_sgd_mse[0],  'for lr: ', lr_sgd)\n",
    "print('Momentum loss after 100 epochs:', min_mntm_mse[0], 'for lr: ', lr_mntm, ', mntm: ',mntm_mntm)\n",
    "print('Nesterov loss after 100 epochs:', min_nesterov_mse[0], 'for lr: ',lr_nesterov, ', mntm: ', nesterov_mntm)\n",
    "print('ADAGRAD loss after 100 epochs: ', min_adagrad_mse[0], ' for lr: ', lr_adagrad)\n",
    "print('ADADELTA loss after 100 epochs:', min_adadelta_mse[0], 'for rho: ', rho_adadelta)\n",
    "print('RMSPROP loss after 100 epochs: ', min_rmsprop_mse[0], 'for lr: ', lr_rmsprop, ', rho: ', rho_rmsprop)\n",
    "print('ADAM loss after 100 epochs:    ', min_adam_mse[0], 'for lr: ', lr_adam, ', beta1: ', beta1_adam, ', beta2: ', beta2_adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x115819c18>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x115696c50>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12,5))\n",
    "plt.plot(range(100), sgd_mse, label = 'SGD')\n",
    "plt.plot(range(100), mntm_mse, label = 'Momentum')\n",
    "plt.plot(range(100), nesterov_mse, label = 'Nesterov')\n",
    "plt.plot(range(100), adagrad_mse, label = 'ADAGRAD')\n",
    "plt.plot(range(100), adadelta_mse, label = 'ADADELTA')\n",
    "plt.plot(range(100), rmsprop_mse, label = 'RMSPROP')\n",
    "plt.plot(range(100), adam_mse, label = 'ADAM')\n",
    "plt.xlim(0,25)\n",
    "# plt.yscale('log')\n",
    "plt.title('MSE loss for each epoch. batch size = 100')\n",
    "plt.xlabel('Number of epochs')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> From the graph we can see that __SGD, Momentum with and without Nestrov__ converge very fast. But looking at the loss __ADAM__ performs better than the others and converges within 10 iterations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
