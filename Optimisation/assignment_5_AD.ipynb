{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 5 - Optimization\n",
    "\n",
    "***\n",
    "**Name**: Payoj Jain\n",
    "***\n",
    "\n",
    "## Goal\n",
    "\n",
    "The goal of this assignment is to use gradient based optimization algorithms to find the minum for the Rosenbrock function and to optmizie the 1 layer MLP network you built in **Assignment 4**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Optimizing the Rosenbrock function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More information can be found here https://en.wikipedia.org/wiki/Rosenbrock_function\n",
    "    \n",
    "**Note: For this assignment, we will choose a = 1 and b = 100**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed \n",
    "import random\n",
    "import unittest\n",
    "\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rosenbrock(a, b, x, y):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    a,b : parameters \n",
    "    x, y: inputs\n",
    "    \n",
    "    Outputs:\n",
    "    out: Rosenbrock function evaluated at x and y\n",
    "    \"\"\"\n",
    "    return (a-x)**2 + b*(y-x**2)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rosenbrock_grad(a, b, x, y):\n",
    "    \"\"\"\n",
    "    Calculate gradient of the rosenbrock function wrt x and y\n",
    "    \n",
    "    Inputs:\n",
    "    a, b: parameters\n",
    "    x, y: inputs\n",
    "    \n",
    "    Outputs:\n",
    "    grad_x, grad_y: Gradients wrt x and y\n",
    "    \"\"\"\n",
    "    grad_x = 2*(x - a) - 4*b*x*(y-x**2)\n",
    "    grad_y = 2*b*(y-x**2)\n",
    "    return grad_x, grad_y "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, you are given skeleton code for optimizing the rosenbrock function using various update rules. Following each function, there are a few function calls with specific hyperparameter choices. The outputs for these will be used to grade your work.\n",
    "\n",
    "Termination condition: \n",
    "\n",
    "1. Reached the n_epochs limit \n",
    "\n",
    "2. The change in value of the function at $x_t,y_t$ and $x_{t+1},y_{t+1}$ is <= tolerance \n",
    "\n",
    "All these functions share the same structure i.e apart from the update rule (and keeping track of past variables) very little changes across these functions.\n",
    "\n",
    "Note: since there is no randomness involved, we expect the outputs to closely match those of our implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rosenbrock_sgd(initial_x, initial_y, a, b, n_epochs, lr, tolerance):\n",
    "    \"\"\"\n",
    "    Use Vanilla SGD to optimize the Rosenbrock function\n",
    "    \n",
    "    Inputs:\n",
    "    initial_x, initial_y : Starting values\n",
    "    a, b : parameters\n",
    "    n_epochs : Maximum no of epochs \n",
    "    lr: Learning rate\n",
    "    tolerance: Tolerance for the error. Terminate if the function value does not change by atleast this much.\n",
    "    \n",
    "    Outputs:\n",
    "    final_x, final_y : Converged point after termination\n",
    "    stop_epoch: Epoch no at which we stop\n",
    "    \"\"\"\n",
    "    \n",
    "    final_x = initial_x\n",
    "    final_y = initial_y\n",
    "    change = function_value = rosenbrock(a,b,initial_x,initial_y)\n",
    "    stop_epoch = 0\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        if(change < tolerance):\n",
    "            print('SGD',final_x, final_y, stop_epoch)\n",
    "            return final_x,final_y,stop_epoch\n",
    "        \n",
    "        grad_x, grad_y = rosenbrock_grad(a,b,final_x,final_y)\n",
    "\n",
    "        final_x = final_x - lr*grad_x\n",
    "        final_y = final_y - lr*grad_y\n",
    "        \n",
    "        new_function_value = rosenbrock(a,b,final_x,final_y)\n",
    "        change = abs(new_function_value - function_value)\n",
    "        function_value = new_function_value\n",
    "        stop_epoch = epoch\n",
    "    return final_x,final_y,n_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rosenbrock_momentum(initial_x, initial_y, a, b, n_epochs, lr, mntm, nesterov, tolerance):\n",
    "    \"\"\"\n",
    "    Use momentum to optimize the Rosenbrock function\n",
    "    \n",
    "    Tip: While implementing nesterov update, you will need the gradient at the next step as well. \n",
    "        Instead, to simplify your implementation, you can use an alternative form of the nesterov \n",
    "        update which only uses the gradient at the current step.\n",
    "        Without nesterov, your update will be -> learning_rate*(gradient + momentum*grad_history)\n",
    "        With nesterov, you update will be -> lr*((1+mntm)*gradient + (mntm)^2 * grad_history)\n",
    "    \n",
    "    Inputs:\n",
    "    initial_x, initial_y : Starting values\n",
    "    a, b : parameters\n",
    "    n_epochs : Maximum no of epochs \n",
    "    lr: Learning rate\n",
    "    mntm: momentum factor\n",
    "    nesterov: True if nesterov update is to be used\n",
    "    tolerance: Tolerance for the error. Terminate if the error does not change by atleast this much.\n",
    "    \n",
    "    Outputs:\n",
    "    final_x, final_y : Converged point after termination\n",
    "    stop_epoch: Epoch no at which we stop\n",
    "    \"\"\"\n",
    "    \n",
    "    final_x = initial_x\n",
    "    final_y = initial_y\n",
    "    change = function_value = rosenbrock(a,b,initial_x,initial_y)\n",
    "    stop_epoch = 0\n",
    "    update_x, update_y = 0, 0\n",
    "\n",
    "    if nesterov:\n",
    "        gamma1 = 1 + mntm\n",
    "        gamma2 = mntm**2\n",
    "    else:\n",
    "        gamma1 = 1\n",
    "        gamma2 = mntm\n",
    "            \n",
    "    for epoch in range(n_epochs):\n",
    "        if(change < tolerance):\n",
    "            print('momentum','nesterov : ',nesterov,final_x, final_y, stop_epoch)\n",
    "            return final_x,final_y,stop_epoch\n",
    "        \n",
    "            \n",
    "        grad_x, grad_y = rosenbrock_grad(a,b,final_x,final_y)\n",
    "        \n",
    "        update_x = lr*(gamma1*grad_x + gamma2*update_x)\n",
    "        update_y = lr*(gamma1*grad_y + gamma2*update_y)\n",
    "                \n",
    "        final_x = final_x - update_x\n",
    "        final_y = final_y - update_y\n",
    "        \n",
    "        new_function_value = rosenbrock(a,b,final_x,final_y)\n",
    "        change = abs(new_function_value - function_value)\n",
    "        function_value = new_function_value\n",
    "        stop_epoch = epoch\n",
    "        \n",
    "    return final_x,final_y,n_epochs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rosenbrock_adagrad(initial_x, initial_y, a, b, n_epochs, lr, eps, tolerance):\n",
    "    \"\"\"\n",
    "    Use Adagrad to optimize the Rosenbrock function\n",
    "    \n",
    "    Inputs:\n",
    "    initial_x, initial_y : Starting values\n",
    "    a, b : parameters\n",
    "    n_epochs : Maximum no of epochs \n",
    "    lr: Learning rate\n",
    "    eps: The fudge factor (used in the denominator of the update to reduce numerical instability)\n",
    "    tolerance: Tolerance for the error. Terminate if the error does not change by atleast this much.\n",
    "    \n",
    "    Outputs:\n",
    "    final_x, final_y : Converged point after termination\n",
    "    stop_epoch: Epoch no at which we stop\n",
    "    \"\"\"\n",
    "    final_x = initial_x\n",
    "    final_y = initial_y\n",
    "    change = function_value = rosenbrock(a,b,initial_x,initial_y)\n",
    "    stop_epoch = 0\n",
    "    r_x, r_y = 0,0\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        if(change < tolerance):\n",
    "            print('adagrad',final_x, final_y, stop_epoch)\n",
    "            return final_x,final_y,stop_epoch\n",
    "        \n",
    "        grad_x, grad_y = rosenbrock_grad(a,b,final_x,final_y)\n",
    "        \n",
    "        r_x = r_x + grad_x*grad_x\n",
    "        r_y = r_y + grad_y*grad_y\n",
    "        \n",
    "        update_x = lr/((eps+r_x)**0.5)*grad_x\n",
    "        update_y = lr/((eps+r_y)**0.5)*grad_y\n",
    "        \n",
    "        final_x = final_x - update_x\n",
    "        final_y = final_y - update_y\n",
    "        \n",
    "        new_function_value = rosenbrock(a,b,final_x,final_y)\n",
    "        change = abs(new_function_value - function_value)\n",
    "        function_value = new_function_value\n",
    "        stop_epoch = epoch\n",
    "        \n",
    "    return final_x,final_y,n_epochs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rosenbrock_adadelta(initial_x, initial_y, a, b, n_epochs, rho, eps, tolerance):\n",
    "    \"\"\"\n",
    "    Use Adadelta to optimize the Rosenbrock function\n",
    "    \n",
    "    Inputs:\n",
    "    initial_x, initial_y : Starting values\n",
    "    a, b : parameters\n",
    "    n_epochs : Maximum no of epochs \n",
    "    rho: Averaging factor\n",
    "    eps: fudging factor (for numerical stability)\n",
    "    tolerance: Tolerance for the error. Terminate if the error does not change by atleast this much.\n",
    "    \n",
    "    Outputs:\n",
    "    final_x, final_y : Converged point after termination\n",
    "    stop_epoch: Epoch no at which we stop\n",
    "    \"\"\"\n",
    "    exponential_decay_update_x_avg = 0\n",
    "    exponential_decay_update_y_avg = 0\n",
    "    \n",
    "    exponential_decay_grad_x_avg = 0\n",
    "    exponential_decay_grad_y_avg = 0\n",
    "    \n",
    "    final_x = initial_x\n",
    "    final_y = initial_y\n",
    "    change = function_value = rosenbrock(a,b,initial_x,initial_y)\n",
    "    stop_epoch = 0\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        if(change < tolerance):\n",
    "            print('adadelta',final_x, final_y, stop_epoch)\n",
    "            return final_x,final_y,stop_epoch\n",
    "        \n",
    "        grad_x, grad_y = rosenbrock_grad(a,b,final_x,final_y)\n",
    "        \n",
    "        exponential_decay_grad_x_avg = rho*exponential_decay_grad_x_avg + (1-rho)*grad_x**2\n",
    "        exponential_decay_grad_y_avg = rho*exponential_decay_grad_y_avg + (1-rho)*grad_y**2\n",
    "        \n",
    "        rms_grad_x  = (exponential_decay_grad_x_avg + eps)**0.5\n",
    "        rms_grad_y  = (exponential_decay_grad_y_avg + eps)**0.5\n",
    "        \n",
    "        rms_update_x = (exponential_decay_update_x_avg + eps)**0.5\n",
    "        rms_update_y = (exponential_decay_update_y_avg + eps)**0.5\n",
    "        \n",
    "        update_x = (rms_update_x/rms_grad_x)*grad_x\n",
    "        update_y = (rms_update_y/rms_grad_y)*grad_y\n",
    "        \n",
    "        final_x = final_x - update_x\n",
    "        final_y = final_y - update_y\n",
    "        \n",
    "        exponential_decay_update_x_avg = rho*exponential_decay_update_x_avg + (1-rho)*update_x**2\n",
    "        exponential_decay_update_y_avg = rho*exponential_decay_update_y_avg + (1-rho)*update_y**2\n",
    "        \n",
    "        new_function_value = rosenbrock(a,b,final_x,final_y)\n",
    "        change = abs(new_function_value - function_value)\n",
    "        function_value = new_function_value\n",
    "        stop_epoch = epoch\n",
    "\n",
    "    return final_x,final_y,n_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rosenbrock_rmsprop(initial_x, initial_y, a, b, n_epochs, lr, rho, eps, tolerance):\n",
    "    \"\"\"\n",
    "    Use RMSprop to optimize the Rosenbrock function\n",
    "    \n",
    "    Inputs:\n",
    "    initial_x, initial_y : Starting values\n",
    "    a, b : parameters\n",
    "    n_epochs : Maximum no of epochs \n",
    "    lr:learning rate\n",
    "    rho: Averaging factor\n",
    "    eps: fudging factor (for numerical stability)\n",
    "    tolerance: Tolerance for the error. Terminate if the error does not change by atleast this much.\n",
    "    \n",
    "    Outputs:\n",
    "    final_x, final_y : Converged point after termination\n",
    "    stop_epoch: Epoch no at which we stop\n",
    "    \"\"\"\n",
    "    exponential_decay_grad_x_avg = 0\n",
    "    exponential_decay_grad_y_avg = 0\n",
    "    \n",
    "    final_x = initial_x\n",
    "    final_y = initial_y\n",
    "    change = function_value = rosenbrock(a,b,initial_x,initial_y)\n",
    "    stop_epoch = 0\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        if(change <= tolerance):\n",
    "            print('rmsprop',final_x, final_y, stop_epoch)\n",
    "            return final_x,final_y,stop_epoch\n",
    "        \n",
    "        grad_x, grad_y = rosenbrock_grad(a,b,final_x,final_y)\n",
    "        \n",
    "        exponential_decay_grad_x_avg = rho*exponential_decay_grad_x_avg + (1-rho)*grad_x**2\n",
    "        exponential_decay_grad_y_avg = rho*exponential_decay_grad_y_avg + (1-rho)*grad_y**2\n",
    "        \n",
    "        \n",
    "        update_x = (lr*grad_x/(eps + exponential_decay_grad_x_avg**0.5))\n",
    "        update_y = (lr*grad_y/(eps + exponential_decay_grad_y_avg**0.5))\n",
    "        \n",
    "        final_x = final_x - update_x\n",
    "        final_y = final_y - update_y\n",
    "        \n",
    "        new_function_value = rosenbrock(a,b,final_x,final_y)\n",
    "        change = abs(new_function_value - function_value)\n",
    "        function_value = new_function_value\n",
    "        stop_epoch = epoch\n",
    "\n",
    "    return final_x,final_y,n_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rosenbrock_adam(initial_x, initial_y, a, b, n_epochs, lr, beta1, beta2, eps, tolerance):\n",
    "    \"\"\"\n",
    "    Use Adam to optimize the Rosenbrock function\n",
    "    \n",
    "    Inputs:\n",
    "    initial_x, initial_y : Starting values\n",
    "    a, b : parameters\n",
    "    n_epochs : Maximum no of epochs \n",
    "    lr: learning rate\n",
    "    beta1, beta2: Averaging factors\n",
    "    eps: fudging factor (for numerical stability)\n",
    "    tolerance: Tolerance for the error. Terminate if the error does not change by atleast this much.\n",
    "    \n",
    "    Outputs:\n",
    "    final_x, final_y : Converged point after termination\n",
    "    stop_epoch: Epoch no at which we stop\n",
    "    \"\"\"\n",
    "    m_x, m_y = 0, 0\n",
    "    \n",
    "    v_x, v_y = 0, 0\n",
    "    \n",
    "    final_x = initial_x\n",
    "    final_y = initial_y\n",
    "    change = function_value = rosenbrock(a,b,initial_x,initial_y)\n",
    "    stop_epoch = 0\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        if(change < tolerance):\n",
    "            print('adam',final_x, final_y, stop_epoch)\n",
    "            return final_x,final_y,stop_epoch\n",
    "        \n",
    "        grad_x, grad_y = rosenbrock_grad(a,b,final_x,final_y)\n",
    "        \n",
    "        m_x = beta1*m_x + (1 - beta1)*grad_x\n",
    "        m_y = beta1*m_y + (1 - beta1)*grad_y\n",
    "        \n",
    "        v_x = beta2*v_x + (1 - beta2)*grad_x**2\n",
    "        v_y = beta2*v_y + (1 - beta2)*grad_y**2\n",
    "        \n",
    "        m_x_avg = m_x /(1 - beta1**(epoch+1))\n",
    "        m_y_avg = m_y /(1 - beta1**(epoch+1))\n",
    "        \n",
    "        v_x_avg = v_x /(1 - beta2**(epoch+1))\n",
    "        v_y_avg = v_y /(1 - beta2**(epoch+1))\n",
    "        \n",
    "        update_x = (lr/(v_x_avg**0.5+eps))*m_x_avg\n",
    "        update_y = (lr/(v_y_avg**0.5+eps))*m_y_avg\n",
    "        \n",
    "        final_x = final_x - update_x\n",
    "        final_y = final_y - update_y\n",
    "        \n",
    "        new_function_value = rosenbrock(a,b,final_x,final_y)\n",
    "        change = abs(new_function_value - function_value)\n",
    "        function_value = new_function_value\n",
    "        stop_epoch = epoch\n",
    "        \n",
    "    return final_x,final_y,n_epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unit Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "......"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adadelta 0.9320850940444184 0.8726938644568357 2341\n",
      "adagrad 0.9601029668996042 0.9217502990713288 8105\n",
      "adam 0.9882985356765367 0.9767314100355323 2191\n",
      "momentum nesterov :  True 0.975228639507472 0.9509702874332484 3253\n",
      "momentum nesterov :  False 0.9656473450926458 0.9323344634810155 5566\n",
      "rmsprop 0.9876101743117767 0.9768227299685097 3172\n",
      "SGD 0.965628504058544 0.932297997695398 5570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 6 tests in 0.070s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "class TestRosenBrock(unittest.TestCase):\n",
    "    def test_sgd(self):\n",
    "        final_x, final_y, stop_epoch = rosenbrock_sgd(0, 0, 1, 100, 1, 0.001, 1e-06)\n",
    "        self.assertAlmostEqual(final_x, 0.002, places = 4)\n",
    "        self.assertAlmostEqual(final_y, 0, places = 4)\n",
    "        self.assertAlmostEqual(stop_epoch, 1, places = 4)\n",
    "\n",
    "        final_x, final_y, stop_epoch = rosenbrock_sgd(0, 0, 1, 100, 5, 0.001, 1e-06)\n",
    "        self.assertAlmostEqual(final_x, 0.009959805751775453, places = 4)\n",
    "        self.assertAlmostEqual(final_y, 2.091e-05, places = 4)\n",
    "        self.assertAlmostEqual(stop_epoch, 5, places = 4)\n",
    "        \n",
    "        final_x, final_y, stop_epoch = rosenbrock_sgd(0, 0, 1, 100, 100000, 0.001, 1e-06)\n",
    "        self.assertAlmostEqual(final_x, 0.965628504058544, places = 4)\n",
    "        self.assertAlmostEqual(final_y, 0.932297997695398, places = 4)\n",
    "        self.assertAlmostEqual(stop_epoch,  5570, places = 4)\n",
    "    \n",
    "    def test_momentum(self):\n",
    "        final_x, final_y, stop_epoch = rosenbrock_momentum(0, 0, 1, 100, 1, 0.001, 0.95, True, 1e-06)\n",
    "        self.assertAlmostEqual(final_x, 0.0039, places = 4)\n",
    "        self.assertAlmostEqual(final_y, 0, places = 4)\n",
    "        self.assertAlmostEqual(stop_epoch, 1, places = 4)\n",
    "        \n",
    "        final_x, final_y, stop_epoch = rosenbrock_momentum(0, 0, 1, 100, 5, 0.001, 0.95, True, 1e-06)\n",
    "        self.assertAlmostEqual(final_x, 0.019358983472059735, places = 4)\n",
    "        self.assertAlmostEqual(final_y, 0.00013646707993741637, places = 4)\n",
    "        self.assertAlmostEqual(stop_epoch, 5, places = 4)\n",
    "        \n",
    "        final_x, final_y, stop_epoch = rosenbrock_momentum(0, 0, 1, 100, 100000, 0.001, 0.95, True, 1e-06)\n",
    "        self.assertAlmostEqual(final_x, 0.975228639507472, places = 4)\n",
    "        self.assertAlmostEqual(final_y, 0.9509702874332484, places = 4)\n",
    "        self.assertAlmostEqual(stop_epoch, 3253, places = 4)\n",
    "        \n",
    "        final_x, final_y, stop_epoch = rosenbrock_momentum(0, 0, 1, 100, 1, 0.001, 0.95, False, 1e-06)\n",
    "        self.assertAlmostEqual(final_x, 0.002, places = 4)\n",
    "        self.assertAlmostEqual(final_y, 0, places = 4)\n",
    "        self.assertAlmostEqual(stop_epoch, 1, places = 4)\n",
    "        \n",
    "        final_x, final_y, stop_epoch = rosenbrock_momentum(0, 0, 1, 100, 5, 0.001, 0.95, False, 1e-06)\n",
    "        self.assertAlmostEqual(final_x, 0.00996736498329375, places = 4)\n",
    "        self.assertAlmostEqual(final_y, 2.0949769804179422e-05, places = 4)\n",
    "        self.assertAlmostEqual(stop_epoch, 5, places = 4)\n",
    "        \n",
    "        final_x, final_y, stop_epoch = rosenbrock_momentum(0, 0, 1, 100, 100000, 0.001, 0.95, False, 1e-06)\n",
    "        self.assertAlmostEqual(final_x, 0.9656473450926458, places = 4)\n",
    "        self.assertAlmostEqual(final_y, 0.9323344634810155, places = 4)\n",
    "        self.assertAlmostEqual(stop_epoch, 5566, places = 4)\n",
    "\n",
    "    def test_adagrad(self):\n",
    "        final_x, final_y, stop_epoch = rosenbrock_adagrad(0, 0, 1, 100, 1, lr = 0.01, eps = 1e-04, tolerance = 1e-06)\n",
    "        self.assertAlmostEqual(final_x, 0.009999875002343702, places = 4)\n",
    "        self.assertAlmostEqual(final_y, 0, places = 4)\n",
    "        self.assertAlmostEqual(stop_epoch, 1, places = 4)\n",
    "\n",
    "        final_x, final_y, stop_epoch = rosenbrock_adagrad(0, 0, 1, 100, 5, lr = 0.01, eps = 1e-04, tolerance = 1e-06)\n",
    "        self.assertAlmostEqual(final_x, 0.0321538388386626, places = 4)\n",
    "        self.assertAlmostEqual(final_y, 0.0007748244054554124, places = 4)\n",
    "        self.assertAlmostEqual(stop_epoch, 5, places = 4)\n",
    "        \n",
    "        final_x, final_y, stop_epoch = rosenbrock_adagrad(0, 0, 1, 100, 100000, lr = 0.01, eps = 1e-04, tolerance = 1e-06)\n",
    "        self.assertAlmostEqual(final_x, 0.9601029668996042, places = 4)\n",
    "        self.assertAlmostEqual(final_y, 0.9217502990713288, places = 4)\n",
    "        self.assertAlmostEqual(stop_epoch, 8105, places = 4)\n",
    "        \n",
    "\n",
    "    def test_adadelta(self):\n",
    "        final_x, final_y, stop_epoch = rosenbrock_adadelta(0, 0, 1, 100, 1, rho = 0.95, eps = 1e-08, tolerance = 1e-06)\n",
    "        self.assertAlmostEqual(final_x, 0.0004472135843196183, places = 4)\n",
    "        self.assertAlmostEqual(final_y, 0, places = 4)\n",
    "        self.assertAlmostEqual(stop_epoch, 1, places = 4)\n",
    "        \n",
    "        final_x, final_y, stop_epoch = rosenbrock_adadelta(0, 0, 1, 100, 5, rho = 0.95, eps = 1e-08, tolerance = 1e-06)\n",
    "        self.assertAlmostEqual(final_x, 0.0022774214255509655, places = 4)\n",
    "        self.assertAlmostEqual(final_y, -0.00020407559768761523, places = 4)\n",
    "        self.assertAlmostEqual(stop_epoch, 5, places = 4)\n",
    "        \n",
    "        final_x, final_y, stop_epoch = rosenbrock_adadelta(0, 0, 1, 100, 10000, rho = 0.95, eps = 1e-08, tolerance = 1e-06)\n",
    "        self.assertAlmostEqual(final_x, 0.9320850940444182, places = 4)\n",
    "        self.assertAlmostEqual(final_y, 0.8726938644568353, places = 4)\n",
    "        self.assertAlmostEqual(stop_epoch, 2341, places = 4)\n",
    "        \n",
    "    def test_rmsprop(self):\n",
    "        final_x, final_y, stop_epoch = rosenbrock_rmsprop(0, 0, 1, 100, 1, lr = 0.001, rho = 0.95, eps = 1e-08, tolerance = 1e-06)\n",
    "        self.assertAlmostEqual(final_x, 0.004472135843196183, places = 4)\n",
    "        self.assertAlmostEqual(final_y, 0, places = 4)\n",
    "        self.assertAlmostEqual(stop_epoch, 1, places = 4)\n",
    "        \n",
    "        final_x, final_y, stop_epoch = rosenbrock_rmsprop(0, 0, 1, 100, 5, lr = 0.001, rho = 0.95, eps = 1e-08, tolerance = 1e-06)\n",
    "        self.assertAlmostEqual(final_x, 0.01471318674752176, places = 4)\n",
    "        self.assertAlmostEqual(final_y, 0.00016279213371358814, places = 4)\n",
    "        self.assertAlmostEqual(stop_epoch, 5, places = 4)\n",
    "        \n",
    "        final_x, final_y, stop_epoch = rosenbrock_rmsprop(0, 0, 1, 100, 100000, lr = 0.001, rho = 0.95, eps = 1e-08, tolerance = 1e-08)\n",
    "        self.assertAlmostEqual(final_x, 0.9876128078500009, places = 4)\n",
    "        self.assertAlmostEqual(final_y,  0.9768279433532969, places = 4)\n",
    "        self.assertAlmostEqual(stop_epoch, 3172, places = 4)\n",
    "\n",
    "    def test_adam(self):\n",
    "        final_x, final_y, stop_epoch = rosenbrock_adam(0, 0, 1, 100, 1, lr = 0.001, beta1 = 0.9, beta2 = 0.999, eps = 1e-08, tolerance = 1e-06)\n",
    "        self.assertAlmostEqual(final_x, 0.000999999995, places = 4)\n",
    "        self.assertAlmostEqual(final_y, 0, places = 4)\n",
    "        self.assertAlmostEqual(stop_epoch, 1, places = 4)\n",
    "        \n",
    "        final_x, final_y, stop_epoch = rosenbrock_adam(0, 0, 1, 100, 5, lr = 0.001, beta1 = 0.9, beta2 = 0.999, eps = 1e-08, tolerance = 1e-06)\n",
    "        self.assertAlmostEqual(final_x, 0.004999562994105255, places = 4)\n",
    "        self.assertAlmostEqual(final_y, -0.0005982751561081573, places = 4)\n",
    "        self.assertAlmostEqual(stop_epoch, 5, places = 4)\n",
    "        \n",
    "        final_x, final_y, stop_epoch = rosenbrock_adam(0, 0, 1, 100, 10000, lr = 0.001, beta1 = 0.9, beta2 = 0.999, eps = 1e-08, tolerance = 1e-06)\n",
    "        self.assertAlmostEqual(final_x, 0.9882985356765371, places = 4)\n",
    "        self.assertAlmostEqual(final_y, 0.9767314100355338, places = 4)\n",
    "        self.assertAlmostEqual(stop_epoch, 2191, places = 4)\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=['first-arg-is-ignored'], exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensor import Tensor\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Implementing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "class MLP(object):\n",
    "    def __init__(self, n_in, n_hid, n_out):\n",
    "\n",
    "        self.l1 = Tensor(np.random.randn(n_in, n_hid))\n",
    "        self.l2 = Tensor(np.random.randn(n_hid, n_out))\n",
    "        \n",
    "        self.state = defaultdict(dict)\n",
    "        \n",
    "        # Initialize an empty dictionary (per pair of weights and biases) for future use\n",
    "        # This dictionary will store various histories needed int he algorithms to be implemented\n",
    "        for i in [1, 2]:\n",
    "            self.state[str(i)] = {}\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "            Implement the forward pass.\n",
    "\n",
    "            params:\n",
    "            X (NxM Tensor):\n",
    "                M dimensional input to be feed forward to the network.\n",
    "\n",
    "            returns:\n",
    "\n",
    "            y_hat (1xN Tensor): activations of the output layer.\n",
    "        \"\"\"\n",
    "\n",
    "        self.act_l1 = X.dot(self.l1).relu()\n",
    "        self.act_l2 = self.act_l1.dot(self.l2)\n",
    "\n",
    "        return self.act_l2\n",
    "\n",
    "    def update(self, opt):\n",
    "        \"\"\"\n",
    "            Implement the update rule for network weights and biases.\n",
    "\n",
    "            return:\n",
    "\n",
    "            none.\n",
    "        \"\"\"\n",
    "        \n",
    "        grad_1, grad_2 = opt.update_grad(self)\n",
    "        self.l1.value += grad_1.value\n",
    "        self.l2.value += grad_2.value\n",
    "        \n",
    "    def get_state_accum(self, param, state_dict, name):\n",
    "        \"\"\"\n",
    "        Return a particular variable associated with a layer. If it has not been initialized \n",
    "        (i.e. on the first iteration), then create that variable with the same dimention as param\n",
    "\n",
    "        Example usage: get_state_accum(W, nn.state[str(layer)], 'W_grad_history') would return the \n",
    "                       gradient in the past iteration (assuming you update W_grad_history in the dictionary every\n",
    "                       epoch)\n",
    "\n",
    "        Inputs:\n",
    "        state_dict: dictionary to be queried \n",
    "        name: property of interest\n",
    "        param: If state_dict[name] is being accessed the first time, we initialize it to be a \n",
    "               vector with the same dimesions as param\n",
    "        \"\"\"\n",
    "        if name not in state_dict:\n",
    "            state_dict[name] = np.zeros_like(param)\n",
    "        return state_dict[name]\n",
    "    \n",
    "    def get_layer_params(self, layer):\n",
    "        \"\"\"\n",
    "        Return aparameters of a particular layer\n",
    "        \"\"\"\n",
    "        if layer == 1:\n",
    "            return self.l1\n",
    "        else:\n",
    "            return self.l2\n",
    "        \n",
    "    def get_layer_grad(self, layer):\n",
    "        \"\"\" Return gradient of a particular layer \"\"\"\n",
    "        if layer == 1:\n",
    "            return self.l1.grad\n",
    "        else:\n",
    "            return self.l2.grad\n",
    "\n",
    "    def zero_grad(self):\n",
    "        \"\"\"\n",
    "            Resets gradients for each layer defined in\n",
    "            the constructor.\n",
    "        \"\"\"\n",
    "        self.l1.zero_grad()\n",
    "        self.l2.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class optimizer():\n",
    "    def __init__(self, update_rule, **kwargs):\n",
    "        \"\"\"\n",
    "        Set self.update \n",
    "        Set parameters from kwargs(variable keyword parameters) according to update type\n",
    "        \"\"\"\n",
    "        self.update = update_rule\n",
    "        \n",
    "        if self.update == \"sgd\":\n",
    "            self.lr = kwargs['lr']\n",
    "        elif self.update == \"momentum\":\n",
    "            self.lr = kwargs['lr']\n",
    "            self.mntm = kwargs['mntm']\n",
    "            self.nesterov = kwargs['nesterov']\n",
    "        elif self.update == \"adagrad\":\n",
    "            self.lr = kwargs['lr']\n",
    "            self.eps = kwargs['eps']\n",
    "        elif self.update == \"adadelta\":\n",
    "            self.rho = kwargs['rho']\n",
    "            self.eps = kwargs['eps']\n",
    "        elif self.update == \"rmsprop\":\n",
    "            self.lr = kwargs['lr']\n",
    "            self.rho = kwargs['rho']\n",
    "            self.eps = kwargs['eps']\n",
    "        elif self.update == \"adam\":\n",
    "            self.lr = kwargs['lr']\n",
    "            self.beta1 = kwargs['beta1']\n",
    "            self.beta2 = kwargs['beta2']\n",
    "            self.eps = kwargs['eps']\n",
    "    \n",
    "    def update_grad(self, model):\n",
    "        \"\"\"\n",
    "        Returns proper update according to the string in self.update\n",
    "        \n",
    "        Input:\n",
    "        model: NN model\n",
    "        \n",
    "        Output:\n",
    "        l1: Update for layer 1\n",
    "        l2: update for layer 2\n",
    "        \"\"\"\n",
    "        if self.update == 'sgd':\n",
    "            l1 = self.update_sgd(1, model, self.lr)\n",
    "            l2 = self.update_sgd(2, model, self.lr)\n",
    "        elif self.update == 'momentum':\n",
    "            l1 = self.update_momentum(1, model, self.lr, self.mntm, self.nesterov)\n",
    "            l2 = self.update_momentum(2, model, self.lr, self.mntm, self.nesterov)\n",
    "        elif self.update == 'adagrad':\n",
    "            l1 = self.update_adagrad(1, model, self.lr, self.eps)\n",
    "            l2 = self.update_adagrad(2, model, self.lr, self.eps)\n",
    "        elif self.update == 'adadelta':\n",
    "            l1 = self.update_adadelta(1, model, self.rho, self.eps)\n",
    "            l2 = self.update_adadelta(2, model, self.rho, self.eps)\n",
    "        elif self.update == 'rmsprop':\n",
    "            l1 = self.update_rmsprop(1, model, self.lr, self.rho, self.eps)\n",
    "            l2 = self.update_rmsprop(2, model, self.lr, self.rho, self.eps)\n",
    "        elif self.update == 'adam':\n",
    "            l1 = self.update_adam(1, model, self.lr, self.beta1, self.beta2, self.eps)\n",
    "            l2 = self.update_adam(2, model, self.lr, self.beta1, self.beta2, self.eps)\n",
    "            \n",
    "        return l1, l2\n",
    "        \n",
    "    def update_sgd(self, layer, model, lr):\n",
    "        \"\"\"\n",
    "        Update function for sgd.\n",
    "        Make sure you update the moving averages for 'layer' inside this function. \n",
    "    \n",
    "        Inputs:\n",
    "        layer: layer for which the update has to be returned\n",
    "        model: NN model to be updated (you need this to access 'state' and update histories)\n",
    "        lr: learning rate\n",
    "\n",
    "        Outputs:\n",
    "        update (Tensor)\n",
    "        \"\"\"\n",
    "        name = 'sgd'\n",
    "        grad = model.get_layer_grad(layer)\n",
    "        update = -lr*grad\n",
    "        model.state[str(layer)][name] = update\n",
    "        return Tensor(update)\n",
    "    \n",
    "    def update_momentum(self, layer, model, lr, mntm, nesterov):\n",
    "        \"\"\"\n",
    "        Update function for momentum.\n",
    "        Make sure you update the past gradients for 'layer' inside this function.\n",
    "\n",
    "        Tip: While implementing nesterov update, you will need the gradient at the next step as well. \n",
    "            Instead, to simplify your implementation, you can use an alternative form of the nesterov \n",
    "            update which only uses the gradient at the current step.\n",
    "            Without nesterov, your update will be -> learning_rate*(gradient + momentum*grad_history)\n",
    "            With nesterov, you update will be -> lr*((1+mntm)*gradient + (mntm)^2 * grad_history)\n",
    "            \n",
    "        Inputs:\n",
    "        layer: layer for which the update has to be returned\n",
    "        model: NN model to be updated (you need this to access 'state' and update histories)\n",
    "        lr: learning rate\n",
    "        mntm: momentum factor\n",
    "        nesterov: if True, function returns nesterov accelerated update\n",
    "\n",
    "        Outputs:\n",
    "        update (Tensor)\n",
    "        \"\"\"\n",
    "        if nesterov:\n",
    "            name = 'momentum nesterov'\n",
    "            gamma1 = 1+mntm\n",
    "            gamma2 = mntm**2\n",
    "        else:\n",
    "            name = 'momentum without nesterov'\n",
    "            gamma1 = 1\n",
    "            gamma2 = mntm\n",
    "            \n",
    "        grad = model.get_layer_grad(layer)\n",
    "        \n",
    "        update_history = model.get_state_accum(grad, model.state[str(layer)], name)\n",
    "        update = -lr*(gamma1*grad + gamma2*update_history)\n",
    "        model.state[str(layer)][name] = update\n",
    "        return Tensor(update)\n",
    "    \n",
    "    def update_adagrad(self, layer, model, lr, eps):\n",
    "        \"\"\"\n",
    "        Update function for adadelta.\n",
    "        Make sure you update the moving averages for 'layer' inside this function. \n",
    "    \n",
    "        Inputs:\n",
    "        layer: layer for which the update has to be returned\n",
    "        model: NN model to be updated (you need this to access 'state' and update histories)\n",
    "        lr: learning rate\n",
    "        eps : fudge factor (for numerical stability)\n",
    "\n",
    "        Outputs:\n",
    "        update (Tensor)\n",
    "        \"\"\"\n",
    "        '''\n",
    "        r_x = r_x + grad_x*grad_x\n",
    "        \n",
    "        update_x = lr/((eps+r_x)**0.5)*grad_x\n",
    "        \n",
    "        '''\n",
    "        name = 'adagrad'\n",
    "        grad = model.get_layer_grad(layer)\n",
    "        \n",
    "        r_history = model.get_state_accum(grad, model.state[str(layer)], name)\n",
    "        r = r_history + grad**2\n",
    "        \n",
    "        model.state[str(layer)][name] = r\n",
    "        \n",
    "        update = -lr/((eps+r)**0.5)*grad\n",
    "\n",
    "        return Tensor(update)\n",
    "        \n",
    "    def update_adadelta(self, layer, model, rho, eps):\n",
    "        \"\"\"\n",
    "        Update function for adadelta.\n",
    "        Make sure you update the moving averages for 'layer' inside this function. \n",
    "    \n",
    "        Inputs:\n",
    "        layer: layer for which the update has to be returned\n",
    "        model: NN model to be updated (you need this to access 'state' and update histories)\n",
    "        rho: averaging factor\n",
    "        eps : fudge factor (for numerical stability)\n",
    "\n",
    "        Outputs:\n",
    "        update (Tensor)\n",
    "        \"\"\"\n",
    "        '''\n",
    "        exponential_decay_grad_x_avg = rho*exponential_decay_grad_x_avg + (1-rho)*grad_x**2\n",
    "        \n",
    "        rms_grad_x  = (exponential_decay_grad_x_avg + eps)**0.5\n",
    "\n",
    "        \n",
    "        rms_update_x = (exponential_decay_update_x_avg + eps)**0.5\n",
    "        \n",
    "        update_x = (rms_update_x/rms_grad_x)*grad_x\n",
    "        \n",
    "        exponential_decay_update_x_avg = rho*exponential_decay_update_x_avg + (1-rho)*update_x**2\n",
    "\n",
    "        \n",
    "        '''\n",
    "        update_name = 'adadelta_update'\n",
    "        grad_name = 'adadelta_grad'\n",
    "        \n",
    "        grad = model.get_layer_grad(layer)\n",
    "        \n",
    "        update_history = model.get_state_accum(grad, model.state[str(layer)], update_name)\n",
    "        grad_history = model.get_state_accum(grad,model.state[str(layer)],grad_name)\n",
    "        \n",
    "        grad_history = rho*grad_history + (1-rho)*grad**2\n",
    "        \n",
    "        rms_grad = (grad_history + eps)**0.5\n",
    "        rms_update = (update_history + eps)**0.5\n",
    "        \n",
    "        update = -(rms_update/rms_grad)*grad\n",
    "        \n",
    "        update_history = rho*update_history + (1-rho)*update**2\n",
    "        \n",
    "        model.state[str(layer)][update_name] = update_history\n",
    "        model.state[str(layer)][grad_name] = grad_history\n",
    "        \n",
    "        return Tensor(update)\n",
    "    \n",
    "    def update_rmsprop(self, layer, model, lr, rho, eps):\n",
    "        \"\"\"\n",
    "        Update function for rmsprop.\n",
    "        Make sure you update the moving averages for 'layer' inside this function. \n",
    "    \n",
    "        Inputs:\n",
    "        layer: layer for which the update has to be returned\n",
    "        model: NN model to be updated (you need this to access 'state' and update histories)\n",
    "        lr: learning rate\n",
    "        rho : averaging factor\n",
    "        eps : fudge factor (for numerical stability)\n",
    "\n",
    "        Outputs:\n",
    "        update (Tensor)\n",
    "        \"\"\"\n",
    "        '''\n",
    "        grad_x, grad_y = rosenbrock_grad(a,b,final_x,final_y)\n",
    "        \n",
    "        exponential_decay_grad_x_avg = rho*exponential_decay_grad_x_avg + (1-rho)*grad_x**2\n",
    "        \n",
    "        update_x = (lr*grad_x/(eps + exponential_decay_grad_x_avg**0.5))\n",
    "\n",
    "        '''\n",
    "        \n",
    "        update_name = 'rmsprop'\n",
    "        \n",
    "        grad = model.get_layer_grad(layer)\n",
    "        \n",
    "        update_history = model.get_state_accum(grad, model.state[str(layer)], update_name)\n",
    "        \n",
    "        update_history = rho*update_history + (1-rho)*grad**2\n",
    "        \n",
    "        model.state[str(layer)][update_name] = update_history\n",
    "        \n",
    "        update = -(lr*grad)/(eps+update_history**0.5)\n",
    "        \n",
    "        return Tensor(update)\n",
    "    \n",
    "    def update_adam(self, layer, model, lr, beta1, beta2, eps):\n",
    "        \"\"\"\n",
    "        Update function for adam.\n",
    "        Make sure you update the moving averages for 'layer' inside this function. \n",
    "    \n",
    "        Inputs:\n",
    "        layer: layer for which the update has to be returned\n",
    "        model: NN model to be updated (you need this to access 'state' and update histories)\n",
    "        lr: learning rate\n",
    "        beta1, beta2 : averaging factors\n",
    "        eps : fudge factor (for numerical stability)\n",
    "\n",
    "        Outputs:\n",
    "        update (Tensor)\n",
    "        \"\"\"\n",
    "        '''\n",
    "        grad_x, grad_y = rosenbrock_grad(a,b,final_x,final_y)\n",
    "        \n",
    "        m_x = beta1*m_x + (1 - beta1)*grad_x\n",
    "        \n",
    "        v_x = beta2*v_x + (1 - beta2)*grad_x**2\n",
    "        \n",
    "        m_x_avg = m_x /(1 - beta1**(epoch+1))\n",
    "        \n",
    "        v_x_avg = v_x /(1 - beta2**(epoch+1))\n",
    "        \n",
    "        update_x = (lr/(v_x_avg**0.5+eps))*m_x_avg\n",
    "        '''\n",
    "        \n",
    "        m_name = 'adam_m'\n",
    "        v_name = 'adam_v'\n",
    "        exp_name = 'epoch_num'\n",
    "        grad = model.get_layer_grad(layer)\n",
    "        \n",
    "        m_history = model.get_state_accum(grad, model.state[str(layer)], m_name)\n",
    "        v_history = model.get_state_accum(grad, model.state[str(layer)], v_name)\n",
    "        \n",
    "        m_history = beta1*m_history + (1 - beta1)*grad\n",
    "        v_history = beta2*v_history + (1 - beta2)*grad**2\n",
    "        \n",
    "        exp = model.get_state_accum(1, model.state[str(layer)], exp_name)\n",
    "        exp += 1\n",
    "        m_avg = m_history/(1 - beta1**exp)\n",
    "        v_avg = v_history/(1 - beta2**exp)\n",
    "        \n",
    "        model.state[str(layer)][m_name] = m_history\n",
    "        model.state[str(layer)][v_name] = v_history\n",
    "        model.state[str(layer)][exp_name] = exp\n",
    "        \n",
    "        update = (-lr/(v_avg**0.5 + eps))*m_avg\n",
    "        \n",
    "        return Tensor(update)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Train the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, X, y, epochs, batch_size, opt):\n",
    "    \"\"\"\n",
    "    Implement the train loop.\n",
    "    \n",
    "    params:\n",
    "    model (MLP): 1-hidden-layer MLP model to be trained.\n",
    "    X (Nx5 ndarray): Training inputs.\n",
    "    y (Nx1 ndarray): Groundtruth labels.\n",
    "    epochs (int): number of epochs for training\n",
    "    batch_size (int)\n",
    "    opt: optimizer object\n",
    "    \n",
    "    returns:\n",
    "    list of errors (one for each epoch)\n",
    "    \"\"\"\n",
    "    # For each minibatch in each epoch, \n",
    "    # 1. do a forward pass on the minibatch\n",
    "    # 2. Compute Loss on minibatch\n",
    "    # 3. Do a backward pass to get gradients\n",
    "    # 4. Update parameters\n",
    "    # 5. Compute error over epoch\n",
    "    \n",
    "\n",
    "    from sklearn.utils import shuffle\n",
    "    mean_squared_error = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        X, y = shuffle(X, y)        \n",
    "        epoch_error = 0\n",
    "\n",
    "        for batch in range(0,X.shape[0],batch_size):\n",
    "            \n",
    "            X_train = Tensor(X[batch:batch+batch_size])\n",
    "            y_train = Tensor(y[batch:batch+batch_size])\n",
    "            \n",
    "            y_hat = model.forward(X_train)\n",
    "            \n",
    "            model.zero_grad()\n",
    "            \n",
    "            mse = (y_train - y_hat).pow(2).mean()\n",
    "            mse.backward()\n",
    "            \n",
    "            model.update(opt)\n",
    "            \n",
    "            epoch_error += mse.value\n",
    "            \n",
    "        mean_error = epoch_error/(X.shape[0]/batch_size)\n",
    "        mean_squared_error.append(mean_error)\n",
    "    return mean_squared_error\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Usage and Tests\n",
    "***\n",
    "\n",
    "**Part 3.A**  \n",
    "\n",
    "Create a model with $I = 5$ inputs, $H = 100$ and $O = 1$ output neuron. Train the model for 100 epochs using the dataset below. Plot the MSE loss for each epoch.\n",
    "\n",
    "Once you have each optimizer working, which performs best given their initial hyperparameters? Does changing one or more hyperpameters yield better performance for a particular optimizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = np.load(\"X.npy\"), np.load(\"y.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.3362]\n"
     ]
    }
   ],
   "source": [
    "model = MLP(X.shape[1], 100, 1)\n",
    "opt = optimizer(\"sgd\", lr = 0.001)\n",
    "mse_sgd = train(model, X, y, 100, 100, opt)\n",
    "print(mse_sgd[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.3699189]\n"
     ]
    }
   ],
   "source": [
    "model = MLP(X.shape[1], 100, 1)\n",
    "opt = optimizer(\"momentum\", lr = 0.001, mntm = 0.9, nesterov = False)\n",
    "mse_mntm = train(model, X, y, 100, 100, opt)\n",
    "print(mse_mntm[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.27612649]\n"
     ]
    }
   ],
   "source": [
    "model = MLP(X.shape[1], 100, 1)\n",
    "opt = optimizer(\"momentum\", lr = 0.001, mntm = 0.9, nesterov = True)\n",
    "mse_nesterov = train(model, X, y, 100, 100, opt)\n",
    "print(mse_nesterov[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.49683922]\n"
     ]
    }
   ],
   "source": [
    "model = MLP(X.shape[1], 100, 1)\n",
    "opt = optimizer(\"adagrad\", lr = 0.01, eps = 1e-06)\n",
    "mse_adagrad = train(model, X, y, 100, 100, opt)\n",
    "print(mse_adagrad[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.2203556]\n"
     ]
    }
   ],
   "source": [
    "model = MLP(X.shape[1], 100, 1)\n",
    "opt = optimizer(\"adadelta\", rho = 0.9, eps = 1e-06)\n",
    "mse_adadelta = train(model, X, y, 100, 100, opt)\n",
    "print(mse_adadelta[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.19202467]\n"
     ]
    }
   ],
   "source": [
    "model = MLP(X.shape[1], 100, 1)\n",
    "opt = optimizer(\"rmsprop\", lr = 0.001, rho = 0.9, eps = 1e-06)\n",
    "mse_rmsprop = train(model, X, y, 100, 100, opt)\n",
    "print(mse_rmsprop[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.55516973]\n"
     ]
    }
   ],
   "source": [
    "model = MLP(X.shape[1], 100, 1)\n",
    "opt = optimizer(\"adam\", lr = 0.001, beta1 = 0.9, beta2 = 0.999, eps = 1e-04)\n",
    "mse_adam = train(model, X, y, 100, 100, opt)\n",
    "print(mse_adam[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1141e4400>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1140bdc18>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12,5))\n",
    "plt.plot(range(100), mse_sgd, label = 'SGD')\n",
    "plt.plot(range(100), mse_mntm, label = 'Momentum')\n",
    "plt.plot(range(100), mse_nesterov, label = 'Nesterov')\n",
    "plt.plot(range(100), mse_adagrad, label = 'ADAGRAD')\n",
    "plt.plot(range(100), mse_adadelta, label = 'ADADELTA')\n",
    "plt.plot(range(100), mse_rmsprop, label = 'RMSPROP')\n",
    "plt.plot(range(100), mse_adam, label = 'ADAM')\n",
    "plt.xlim(0,50)\n",
    "# plt.ylim(0,300)\n",
    "# plt.yscale('log')\n",
    "plt.title('MSE loss for each epoch. batch size = 100')\n",
    "plt.xlabel('Number of epochs')\n",
    "plt.ylabel('log(MSE)')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
